{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQQJFvYYJms8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "from torchvision.datasets import MNIST, CIFAR10, FashionMNIST\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
        "from torch.utils.data import DataLoader\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Data Loader\n",
        "class DataLoaderManager:\n",
        "  \"\"\"\n",
        "  DataLoader manager class to handle data loading for various datasets.\n",
        "  \"\"\"\n",
        "  def __init__(self, dataset_name, train_batch_size=50000, test_batch_size=10000):\n",
        "        \"\"\"\n",
        "        Initializes the DataLoaderManager with specified dataset and batch sizes.\n",
        "\n",
        "        Args:\n",
        "        dataset_name (str): Name of the dataset ('MNIST', 'CIFAR10', 'FashionMNIST').\n",
        "        train_batch_size (int): Batch size for the training dataset.\n",
        "        test_batch_size (int): Batch size for the test dataset.\n",
        "        \"\"\"\n",
        "        self.dataset_name = dataset_name\n",
        "        self.train_batch_size = train_batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "        self.transform = self.get_transforms()\n",
        "\n",
        "  def get_transforms(self):\n",
        "        \"\"\"\n",
        "        Returns the appropriate transformation for each dataset.\n",
        "\n",
        "        Returns:\n",
        "        torchvision.transforms.Compose: Transformation pipeline.\n",
        "        \"\"\"\n",
        "        if self.dataset_name == 'CIFAR10':\n",
        "            return Compose([\n",
        "                ToTensor(),\n",
        "                Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "            ])\n",
        "        else:  # Default to MNIST and FashionMNIST normalization\n",
        "            return Compose([\n",
        "                ToTensor(),\n",
        "                Normalize((0.1307,), (0.3081,)),\n",
        "                Lambda(lambda x: torch.flatten(x))\n",
        "            ])\n",
        "\n",
        "  def load_dataset(self, train=True):\n",
        "        \"\"\"\n",
        "        Loads the specified dataset.\n",
        "\n",
        "        Args:\n",
        "        train (bool): If True, load training dataset; otherwise, load test dataset.\n",
        "\n",
        "        Returns:\n",
        "        Dataset: The requested dataset.\n",
        "        \"\"\"\n",
        "        if self.dataset_name == 'MNIST':\n",
        "            return MNIST('./data/', train=train, download=True, transform=self.transform)\n",
        "        elif self.dataset_name == 'CIFAR10':\n",
        "            return CIFAR10('./data/', train=train, download=True, transform=self.transform)\n",
        "        elif self.dataset_name == 'FashionMNIST':\n",
        "            return FashionMNIST('./data/', train=train, download=True, transform=self.transform)\n",
        "\n",
        "  def get_data_loaders(self):\n",
        "        \"\"\"\n",
        "        Creates and returns data loaders for training and testing datasets.\n",
        "\n",
        "        Returns:\n",
        "        tuple: Tuple containing the training and testing data loaders.\n",
        "        \"\"\"\n",
        "        train_loader = DataLoader(self.load_dataset(train=True),\n",
        "                                  batch_size=self.train_batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(self.load_dataset(train=False),\n",
        "                                 batch_size=self.test_batch_size, shuffle=False)\n",
        "        return train_loader, test_loader"
      ],
      "metadata": {
        "id": "pkZyCUsRKSHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def overlay_y_on_x(x, y, classes=10):\n",
        "    \"\"\"\n",
        "    Modify the input tensor x by zeroing out the first 'classes' pixels in the width\n",
        "    of the first channel of each image in the batch, and set the pixel corresponding\n",
        "    to the label y to the maximum value in the tensor x.\n",
        "\n",
        "    Args:\n",
        "    x (torch.Tensor): The input tensor, expected to be 4D (batch, channels, height, width).\n",
        "    y (torch.Tensor): The labels corresponding to each item in the batch.\n",
        "    classes (int): The number of classes or width of the area to be zeroed and used for encoding.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: The modified tensor.\n",
        "    \"\"\"\n",
        "    x_ = x.clone()  # Clone the tensor to avoid modifying the original in place.\n",
        "    max_value = x.max()  # Get the maximum value from the entire tensor.\n",
        "    x_[range(x.shape[0]), :, 0, :classes] *= 0.0  # Zero out the first 'classes' pixels in the width.\n",
        "    x_[range(x.shape[0]), :, 0, y] = max_value  # Set the pixel at the label index to the maximum value.\n",
        "    return x_\n",
        "    # x_ = x.clone()  # Clone the tensor to avoid modifying the original in place.\n",
        "    # max_value = x.max()  # Get the maximum value from the entire tensor.\n",
        "\n",
        "    # # Check if the tensor is 2D (flattened images), and reshape it to 4D if necessary\n",
        "    # if x_.dim() == 2:\n",
        "    #     # Assuming images are 28x28 after flattening to 784\n",
        "    #     batch_size = x_.shape[0]\n",
        "    #     height = width = int((x_.shape[1] // classes) ** 0.5)  # Simplified square root calculation\n",
        "    #     x_ = x_.view(batch_size, 1, height, width)\n",
        "\n",
        "    # # Zero out the first 'classes' pixels in the width\n",
        "    # x_[:, :, 0, :classes] *= 0\n",
        "    # # Set the pixel at the label index to the maximum value\n",
        "    # for idx in range(x_.shape[0]):\n",
        "    #     if y[idx] < classes:\n",
        "    #         x_[idx, :, 0, y[idx]] = max_value\n",
        "\n",
        "    # return x_"
      ],
      "metadata": {
        "id": "O0rkJDiBLLMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def overlay_y_on_x_1D(x, y):\n",
        "    \"\"\"Replace the first 10 pixels of data [x] with one-hot-encoded label [y]\n",
        "    \"\"\"\n",
        "    x_ = x.clone()\n",
        "    x_[:, :10] *= 0.0\n",
        "    x_[range(x.shape[0]), y] = x.max()\n",
        "    return x_"
      ],
      "metadata": {
        "id": "08-0degjOEHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_y_neg(y, num_classes=10):\n",
        "    \"\"\"\n",
        "    Generates negative labels for a batch of labels by ensuring each negative label\n",
        "    is different from the original label. Used in contrastive learning setups.\n",
        "\n",
        "    Args:\n",
        "    y (torch.Tensor): A tensor containing a batch of labels.\n",
        "    num_classes (int): The total number of classes.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: A tensor containing a batch of negative labels.\n",
        "    \"\"\"\n",
        "    y_neg = y.clone()  # Clone the original labels to create a new tensor for negative labels.\n",
        "    for idx, y_samp in enumerate(y):\n",
        "        allowed_indices = list(range(num_classes))  # Create a list of all possible class indices.\n",
        "        allowed_indices.remove(y_samp.item())  # Remove the original label to ensure the negative label is different.\n",
        "        # Randomly select a new label from the remaining allowed indices.\n",
        "        y_neg[idx] = torch.tensor(allowed_indices)[torch.randint(len(allowed_indices), size=(1,))].item()\n",
        "    return y_neg"
      ],
      "metadata": {
        "id": "1nsqhfGfO_W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A convolutional layer class that implements the Forward-Forward Algorithm for distinguishing\n",
        "    between positive and negative samples by adjusting the layer weights based on a specified threshold.\n",
        "\n",
        "    Attributes:\n",
        "        conv (nn.Conv2d): Convolutional layer.\n",
        "        relu (nn.ReLU): Rectified Linear Unit activation function.\n",
        "        opt (Adam): Optimizer for updating layer weights.\n",
        "        threshold (float): Threshold for differentiating positive from negative samples.\n",
        "        num_epochs (int): Number of epochs for the custom training loop.\n",
        "        log_interval (int): Interval at which to log training progress.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0, bias=True, lr=0.01, threshold=0.1, num_epochs=100, log_interval=10):\n",
        "        \"\"\"\n",
        "        Initializes the ConvLayer with convolutional operation and training parameters.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): Number of channels in the input.\n",
        "            out_channels (int): Number of channels produced by the convolution.\n",
        "            kernel_size (int or tuple): Size of the convolving kernel.\n",
        "            stride (int or tuple): Stride of the convolution.\n",
        "            padding (int or tuple): Zero-padding added to both sides of the input.\n",
        "            bias (bool): If True, adds a learnable bias to the output.\n",
        "            lr (float): Learning rate for the optimizer.\n",
        "            threshold (float): Goodness threshold for differentiating samples.\n",
        "            num_epochs (int): Total number of training epochs.\n",
        "            log_interval (int): Interval for logging training progress.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.opt = Adam(self.parameters(), lr=lr)\n",
        "        self.threshold = threshold\n",
        "        self.num_epochs = num_epochs\n",
        "        self.log_interval = log_interval\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the ConvLayer.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Activated output after the convolution and normalization.\n",
        "        \"\"\"\n",
        "        # L2 normalization on the input tensor\n",
        "        x_direction = x / (x.norm(p=2, dim=1, keepdim=True) + 1e-6)\n",
        "        x = self.conv(x_direction)\n",
        "        return self.relu(x)\n",
        "\n",
        "    def custom_train(self, x_pos, x_neg):\n",
        "        \"\"\"\n",
        "        Custom training loop to adjust weights based on the positive and negative samples.\n",
        "\n",
        "        Args:\n",
        "            x_pos (torch.Tensor): Positive samples.\n",
        "            x_neg (torch.Tensor): Negative samples.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Detached tensors of forward passes from positive and negative samples after training.\n",
        "        \"\"\"\n",
        "        loss_values, g_pos_values, g_neg_values = [], [], []\n",
        "\n",
        "        for i in range(self.num_epochs):\n",
        "            g_pos = self.forward(x_pos).pow(2).mean(dim=(1, 2, 3)).unsqueeze(0)\n",
        "            g_neg = self.forward(x_neg).pow(2).mean(dim=(1, 2, 3)).unsqueeze(0)\n",
        "            loss = torch.log(1 + torch.exp(torch.cat([-g_pos + self.threshold, g_neg - self.threshold]))).mean()\n",
        "            self.opt.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "            self.opt.step()\n",
        "\n",
        "            if i % self.log_interval == 0:\n",
        "                loss_values.append(loss.item())\n",
        "                g_pos_values.append(g_pos.mean().item())\n",
        "                g_neg_values.append(g_neg.mean().item())\n",
        "                self._plot_training_progress(loss_values, g_pos_values, g_neg_values, i)\n",
        "\n",
        "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()\n",
        "\n",
        "    def _plot_training_progress(self, loss_values, g_pos_values, g_neg_values, step):\n",
        "        \"\"\"\n",
        "        Plots the training progress.\n",
        "\n",
        "        Args:\n",
        "            loss_values (list): Recorded loss values.\n",
        "            g_pos_values (list): Recorded values of positive samples' goodness.\n",
        "            g_neg_values (list): Recorded values of negative samples' goodness.\n",
        "            step (int): Current training step.\n",
        "        \"\"\"\n",
        "        clear_output(wait=True)\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.subplot(3, 1, 1)\n",
        "        plt.plot(loss_values, color='blue')\n",
        "        plt.title(f\"Loss during training at step {step}\")\n",
        "\n",
        "        plt.subplot(3, 1, 2)\n",
        "        plt.plot(g_pos_values, color='green')\n",
        "        plt.title(\"g_pos during training\")\n",
        "\n",
        "        plt.subplot(3, 1, 3)\n",
        "        plt.plot(g_neg_values, color='red')\n",
        "        plt.title(\"g_neg during training\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "-B_2E4xxxv6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer(nn.Linear):\n",
        "    \"\"\"\n",
        "    A custom layer class that extends nn.Linear for the purpose of training with\n",
        "    a specific loss function that discriminates between positive and negative samples.\n",
        "\n",
        "    Attributes:\n",
        "        relu (nn.ReLU): Activation function.\n",
        "        opt (Adam): Optimizer for the layer parameters.\n",
        "        threshold (float): Threshold value to separate positive and negative samples.\n",
        "        num_epochs (int): Number of training epochs.\n",
        "        loss_values (list): List to store loss values for visualization.\n",
        "        g_pos_values (list): List to store values of goodness metric for positive samples.\n",
        "        g_neg_values (list): List to store values of goodness metric for negative samples.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features,kernel_size=3,stride=None,padding=None, bias=True, device=None, dtype=None, num_epochs=1000, threshold=2.0, lr=0.03):\n",
        "        super().__init__(in_features, out_features, bias=bias, device=device, dtype=dtype)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.opt = Adam(self.parameters(), lr=lr)\n",
        "        self.threshold = threshold\n",
        "        self.num_epochs = num_epochs\n",
        "        self.loss_values = []\n",
        "        self.g_pos_values = []\n",
        "        self.g_neg_values = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the layer which normalizes the input before applying the linear transformation and activation.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after applying linear transformation and ReLU activation.\n",
        "        \"\"\"\n",
        "        x_normalized = x / (x.norm(p=2, dim=1, keepdim=True) + 1e-4)\n",
        "        return self.relu(torch.mm(x_normalized, self.weight.T) + self.bias.unsqueeze(0))\n",
        "\n",
        "    def custom_train(self, x_pos, x_neg):\n",
        "        \"\"\"\n",
        "        Custom training loop that adjusts weights based on the comparison of positive and negative samples\n",
        "        against a set threshold.\n",
        "\n",
        "        Args:\n",
        "            x_pos (torch.Tensor): Positive samples.\n",
        "            x_neg (torch.Tensor): Negative samples.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Detached tensors of the outputs from the final forward pass of positive and negative samples.\n",
        "        \"\"\"\n",
        "        for i in tqdm(range(self.num_epochs)):\n",
        "            g_pos = self.forward(x_pos).pow(2).mean(dim=1)\n",
        "            g_neg = self.forward(x_neg).pow(2).mean(dim=1)\n",
        "            loss = torch.log(1 + torch.exp(torch.cat([-g_pos + self.threshold, g_neg - self.threshold]))).mean()\n",
        "            self.opt.zero_grad()\n",
        "            loss.backward()\n",
        "            self.opt.step()\n",
        "\n",
        "            if i % 10 == 0:  # Log at every 10th epoch\n",
        "                self.loss_values.append(loss.item())\n",
        "                self.g_pos_values.append(g_pos.mean().item())\n",
        "                self.g_neg_values.append(g_neg.mean().item())\n",
        "                # self.plot_training_progress()\n",
        "\n",
        "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()"
      ],
      "metadata": {
        "id": "JGWls-R_x9XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    \"\"\"\n",
        "    A neural network that incorporates both convolutional and fully connected layers,\n",
        "    specifically designed to distinguish between positive and negative samples using\n",
        "    the Forward-Forward Algorithm. This network adjusts to different image sizes.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, num_classes=10, input_size=28):\n",
        "        super(Net, self).__init__()\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            ConvLayer(input_channels, 16, kernel_size=5, padding=2),\n",
        "            ConvLayer(16, 32, kernel_size=5, stride=2, padding=2),\n",
        "            ConvLayer(32, 64, kernel_size=5, stride=2, padding=2),\n",
        "        )\n",
        "\n",
        "        # The number of output features from the last conv layer needs to be determined dynamically\n",
        "        # Dummy input to calculate the size of the feature maps after conv layers\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, input_channels, input_size, input_size)  # Minimal size for CIFAR-10 and MNIST\n",
        "            output_features = self.feature_extractor(dummy_input).numel()\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            Layer(output_features, 128),\n",
        "            Layer(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network, applying each layer to the input data.\n",
        "        \"\"\"\n",
        "        x = self.feature_extractor(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        Predict function to compute the goodness per label for each sample in the batch.\n",
        "        \"\"\"\n",
        "        goodness_per_label = []\n",
        "        for label in range(10):  # Assuming 10 possible classes\n",
        "            h = overlay_y_on_x(x, label)\n",
        "            goodness = []\n",
        "            for layer in self.feature_extractor.children():\n",
        "                h = layer(h)\n",
        "            for layer in self.classifier.children():\n",
        "                h = layer(h)\n",
        "            goodness.append((h.pow(2).sum() / h.numel()).unsqueeze(0))\n",
        "            goodness_per_label.append(torch.sum(torch.stack(goodness)).unsqueeze(0))\n",
        "        goodness_per_label = torch.cat(goodness_per_label, 0)\n",
        "        return goodness_per_label.argmax(0)\n",
        "\n",
        "    def custom_train(self, x_pos, x_neg):\n",
        "        \"\"\"\n",
        "        Custom training method that adjusts network weights layer-by-layer based on the Forward-Forward Algorithm.\n",
        "        \"\"\"\n",
        "        h_pos, h_neg = x_pos, x_neg\n",
        "        for i, layer in enumerate(list(self.feature_extractor) + list(self.classifier)):\n",
        "            if hasattr(layer, 'custom_train'):\n",
        "                print(f\"Training layer: {i}\")\n",
        "                h_pos, h_neg = layer.custom_train(h_pos, h_neg)\n",
        "            else:\n",
        "                h_pos = layer(h_pos)\n",
        "                h_neg = layer(h_neg)\n",
        "        return h_pos, h_neg"
      ],
      "metadata": {
        "id": "fI9swb_m_inp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Net(torch.nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.layers = nn.Sequential(\n",
        "#             Layer(3, 64, kernel_size=5, padding=2),  # Layer 0\n",
        "#             nn.ReLU(inplace=True),\n",
        "\n",
        "#             Layer(64, 128, kernel_size=5, padding=2, stride=2),  # Layer 2\n",
        "#             nn.ReLU(inplace=True),\n",
        "\n",
        "#             Layer(128, 256, kernel_size=5, padding=2, stride=2),  # Layer 4\n",
        "#             nn.ReLU(inplace=True),\n",
        "\n",
        "#             Layer(256, 512, kernel_size=5, padding=2, stride=2),  # Layer 6\n",
        "#             nn.ReLU(inplace=True),\n",
        "\n",
        "#             Layer(512, 1024, kernel_size=5, padding=2, stride=2),  # Layer 8\n",
        "#             nn.ReLU(inplace=True),\n",
        "\n",
        "#             Layer(1024, 2048, kernel_size=5, padding=2, stride=2),  # Layer 10\n",
        "#             nn.ReLU(inplace=True),\n",
        "\n",
        "#             Layer(2048, 10, kernel_size=1, stride=1),  # Layer 12\n",
        "#             nn.ReLU(inplace=True),\n",
        "#         )\n",
        "#     def forward(self, x):\n",
        "#         return self.layers(x)\n",
        "\n",
        "#     def predict(self, x):\n",
        "#         goodness_per_label = []\n",
        "#         for label in range(10):\n",
        "#             h = overlay_y_on_x(x, label)\n",
        "#             goodness = []\n",
        "#             for layer in self.layers:\n",
        "#                 h = layer(h)\n",
        "#                 goodness += [(h.pow(2).sum() / h.numel()).unsqueeze(0)]\n",
        "#             goodness_per_label += [torch.sum(torch.stack(goodness)).unsqueeze(0)]\n",
        "#         goodness_per_label = torch.cat(goodness_per_label, 0)\n",
        "#         return goodness_per_label.argmax(0)\n",
        "\n",
        "#     def custom_train(self, x_pos, x_neg):\n",
        "#         h_pos, h_neg = x_pos, x_neg\n",
        "#         for i, layer in enumerate(self.layers):\n",
        "#             print(\"training layer: \", i)\n",
        "#             if isinstance(layer, Layer):  # only call custom_train on instances of the Layer class\n",
        "#                 h_pos, h_neg = layer.custom_train(h_pos, h_neg)\n",
        "#             elif isinstance(layer, ConvLayer):  # only call custom_train on instances of the FullyConnectedLayer class\n",
        "#                 h_pos, h_neg = layer.custom_train(h_pos, h_neg)\n",
        "#             else:  # for other layers, just pass the data through\n",
        "#                 h_pos = layer(h_pos)\n",
        "#                 h_neg = layer(h_neg)"
      ],
      "metadata": {
        "id": "KrgpH_gye2UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Net(nn.Module):\n",
        "#     \"\"\"\n",
        "#     A neural network that incorporates both convolutional and fully connected layers specifically designed\n",
        "#     to distinguish between positive and negative samples using the Forward-Forward Algorithm.\n",
        "#     \"\"\"\n",
        "#     def __init__(self, input_channels, num_classes=10):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.feature_extractor = nn.Sequential(\n",
        "#             ConvLayer(input_channels, 16, kernel_size=5, padding=2),  # Assume input is image data\n",
        "#             ConvLayer(16, 32, kernel_size=5, stride=2, padding=2),\n",
        "#             ConvLayer(32, 64, kernel_size=5, stride=2, padding=2),\n",
        "#         )\n",
        "\n",
        "#         self.classifier = nn.Sequential(\n",
        "#             nn.Flatten(),\n",
        "#             Layer(64 * 7 * 7, 128),  # Adjust the flattened size according to input dimension\n",
        "#             Layer(128, num_classes)\n",
        "#         )\n",
        "\n",
        "#         self.layers = nn.ModuleList([*self.feature_extractor, *self.classifier])  # Consolidate all layers\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         \"\"\"\n",
        "#         Forward pass through the network, applying each layer to the input data.\n",
        "\n",
        "#         Args:\n",
        "#             x (torch.Tensor): The input tensor to the network.\n",
        "\n",
        "#         Returns:\n",
        "#             torch.Tensor: The output tensor from the network.\n",
        "#         \"\"\"\n",
        "#         for layer in self.layers:\n",
        "#             x = layer(x)\n",
        "#         return x\n",
        "\n",
        "#     def predict(self, x):\n",
        "#         \"\"\"\n",
        "#         Predict function to compute the goodness per label for each sample in the batch.\n",
        "\n",
        "#         Args:\n",
        "#             x (torch.Tensor): The input tensor.\n",
        "\n",
        "#         Returns:\n",
        "#             torch.Tensor: The tensor containing the predicted class labels.\n",
        "#         \"\"\"\n",
        "#         goodness_per_label = []\n",
        "#         for label in range(10):  # Assuming 10 possible classes\n",
        "#             h = overlay_y_on_x(x, label)\n",
        "#             goodness = []\n",
        "#             for layer in self.layers:\n",
        "#                 h = layer(h)\n",
        "#                 goodness.append((h.pow(2).sum() / h.numel()).unsqueeze(0))\n",
        "#             goodness_per_label.append(torch.sum(torch.stack(goodness)).unsqueeze(0))\n",
        "#         goodness_per_label = torch.cat(goodness_per_label, 0)\n",
        "#         return goodness_per_label.argmax(0)\n",
        "\n",
        "#     def custom_train(self, x_pos, x_neg):\n",
        "#         \"\"\"\n",
        "#         Custom training method that adjusts network weights layer-by-layer based on the Forward-Forward Algorithm.\n",
        "\n",
        "#         Args:\n",
        "#             x_pos (torch.Tensor): Positive samples.\n",
        "#             x_neg (torch.Tensor): Negative samples.\n",
        "#         \"\"\"\n",
        "#         h_pos, h_neg = x_pos, x_neg\n",
        "#         for i, layer in enumerate(self.layers):\n",
        "#             if hasattr(layer, 'custom_train'):\n",
        "#                 print(f\"Training layer: {i}\")\n",
        "#                 h_pos, h_neg = layer.custom_train(h_pos, h_neg)\n",
        "#             else:\n",
        "#                 h_pos = layer(h_pos)\n",
        "#                 h_neg = layer(h_neg)\n",
        "#         # h_pos, h_neg = x_pos, x_neg\n",
        "#         # # layers = list(self.feature_extractor) + list(self.classifier)\n",
        "#         # for i, layer in enumerate(self.layers):\n",
        "#         #     if hasattr(layer, 'train'):\n",
        "#         #         print(f'Training layer {i}...')\n",
        "#         #         if isinstance(layer, Layer):  # only call custom_train on instances of the Layer class\n",
        "#         #         h_pos, h_neg = layer.custom_train(h_pos, h_neg)\n",
        "#         #         # h_pos, h_neg = layer.train(h_pos, h_neg)\n",
        "#         #         # layer.train() # Put the layer in training mode\n",
        "#         #         # h_pos = layer(h_pos)  # Apply the layer to positive samples\n",
        "#         #         # h_neg = layer(h_neg)  # Apply the layer to negative samples\n",
        "#         #     else:\n",
        "#         #         h_pos = layer(h_pos)\n",
        "#         #         h_neg = layer(h_neg)"
      ],
      "metadata": {
        "id": "Sk0DIvCgy2eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_sample(data, name='', idx=0):\n",
        "    \"\"\"\n",
        "    Visualizes a single sample from the provided dataset, reshaping it into a 28x28 image\n",
        "    if necessary, and displaying it with a title.\n",
        "\n",
        "    Args:\n",
        "        data (torch.Tensor): The dataset containing the samples.\n",
        "        name (str): The title label for the image.\n",
        "        idx (int): The index of the sample in the dataset to visualize.\n",
        "    \"\"\"\n",
        "    # Assuming data is a PyTorch tensor and the image size is 28x28.\n",
        "    # This will need to be adjusted if your data dimensions or tensor backend differ.\n",
        "    reshaped = data[idx].cpu().reshape(28, 28)\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.title(name)\n",
        "    plt.imshow(reshaped, cmap=\"gray\")\n",
        "    plt.axis('off')  # Optional: Turn off the axis.\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "eA007ucY6oEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available and set the default device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "u6frTBkP7MjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the network with appropriate dimensions for MNIST (1 channel input, 10 classes)\n",
        "net = Net()\n",
        "net.to(device)  # Move the network to the configured device"
      ],
      "metadata": {
        "id": "9KSakQ__fdPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize the network with appropriate dimensions for MNIST (1 channel input, 10 classes)\n",
        "# net = Net(input_channels=3, num_classes=10, input_size=32)\n",
        "# net.to(device)  # Move the network to the configured device\n"
      ],
      "metadata": {
        "id": "Co1Qu6WZ8qFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize data loaders\n",
        "data_manager = DataLoaderManager(\"CIFAR10\")\n",
        "train_loader, test_loader = data_manager.get_data_loaders()\n"
      ],
      "metadata": {
        "id": "7cwDluzx8tky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch a single batch of data\n",
        "x, y = next(iter(train_loader))\n",
        "x, y = x.to(device), y.to(device)  # Move data to the device\n"
      ],
      "metadata": {
        "id": "DclkHCoc8xXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "niWV2zwZPHCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "ktma4o6bPIbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create positive and negative samples\n",
        "x_pos = overlay_y_on_x(x, y)\n",
        "y_neg = get_y_neg(y)\n",
        "x_neg = overlay_y_on_x(x, y_neg)"
      ],
      "metadata": {
        "id": "WUzkqbG882m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(5, 3, figsize=(10, 10))\n",
        "\n",
        "# Define a dictionary to map class indices to class names (replace this with your actual classes)\n",
        "class_dict = {i: 'class_' + str(i) for i in range(10)}\n",
        "\n",
        "for i in range(5):\n",
        "    img = x[i].cpu().numpy().transpose(1,2,0)\n",
        "    pos_img = x_pos[i].cpu().numpy().transpose(1,2,0)\n",
        "    neg_img = x_neg[i].cpu().numpy().transpose(1,2,0)\n",
        "\n",
        "    axs[i, 0].imshow(img)\n",
        "    axs[i, 0].set_title('Original: ' + class_dict[int(y[i])] + '\\n Shape: ' + str(img.shape))\n",
        "\n",
        "    axs[i, 1].imshow(pos_img)\n",
        "    axs[i, 1].set_title('Positive: ' + class_dict[int(y[i])] + '\\n Shape: ' + str(pos_img.shape))\n",
        "\n",
        "    axs[i, 2].imshow(neg_img)\n",
        "    axs[i, 2].set_title('Negative: ' + class_dict[int(y_neg[i])] + '\\n Shape: ' + str(neg_img.shape))\n",
        "\n",
        "for ax in axs.flat:\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()  # Adjusts subplot params so that subplots are nicely fit in the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SXyNBFibPrAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for data, name in zip([x, x_pos, x_neg], ['orig', 'pos', 'neg']):\n",
        "#   visualize_sample(data, name)"
      ],
      "metadata": {
        "id": "Br_x1GFf9GlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the network\n",
        "net.custom_train(x_pos, x_neg)"
      ],
      "metadata": {
        "id": "CSagbCcG9O8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('train error:', 1.0 - net.predict(x).eq(y).float().mean().item())"
      ],
      "metadata": {
        "id": "YZH9di4Z9epx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "00oUnP3pPELr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}