{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeyE-1RjSjNm"
      },
      "source": [
        "# Forward Forward Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH5pgossSjNo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from tensorflow.python.keras import layers, Model\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from typing import Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnMXygtjSjNp"
      },
      "source": [
        "## Label Manipulator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRVxjvutSjNp"
      },
      "outputs": [],
      "source": [
        "class LabelManipulator:\n",
        "    \"\"\"\n",
        "    A class to handle label manipulation tasks such as generating negative labels\n",
        "    for contrastive divergance and overlaying labels onto input data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes: int = 10, device: 'str' = 'cpu'):\n",
        "        \"\"\"\n",
        "        Initialize the LabelManipulator Class.\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): The number of possible classes. Default value is 10.\n",
        "            device (str) : The device to perform computation. Default is cpu.\n",
        "        \"\"\"\n",
        "        self.num_classes = num_classes\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def get_y_neg(self, y: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Generates Negative Labels for contrastive divergence training.\n",
        "\n",
        "\n",
        "        Args:\n",
        "            y (tf.Tensor): A tensor of ground truth labels.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: A tensor of negative labels, where each label is replaced\n",
        "                        with a randomly choosen incorrect label from the same class set.\n",
        "        \"\"\"\n",
        "        # Initialize an empty tensor for the negative labels\n",
        "        y_neg = tf.identity(y)\n",
        "\n",
        "        # Generate a negative label for each sample\n",
        "        for idx, y_samp in enumerate(y):\n",
        "            # Check if `y_samp` is already a NumPy object\n",
        "            if isinstance(y_samp, np.ndarray):\n",
        "                y_samp = y_samp.flatten()[0]  # If NumPy array, directly access its scalar value\n",
        "            else:\n",
        "                y_samp = y_samp.numpy().flatten()[0]  # If it's a TensorFlow tensor, convert to NumPy first\n",
        "\n",
        "            allowed_indices = list(range(self.num_classes))\n",
        "            allowed_indices.remove(int(y_samp))  # Remove the true label\n",
        "            negative_label = np.random.choice(allowed_indices)  #Pick a random negative label\n",
        "            # Reshape the update value to match y_neg's shape\n",
        "            negative_label = tf.constant([negative_label], dtype=y_neg.dtype)\n",
        "\n",
        "            y_neg = tf.tensor_scatter_nd_update(y_neg, [[idx]], [negative_label])\n",
        "\n",
        "\n",
        "        return y_neg\n",
        "\n",
        "\n",
        "    def overlay_y_on_x(self, x: tf.Tensor, y: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Overlays label information onto the input data tensor by marking\n",
        "        the label in a certain position of the tensor.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): Input tensor of shape (batch_size, channels, height, width).\n",
        "            y (tf.Tensor): A tensor of ground truth labels.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: A tensor where the label information is overlaid onto the\n",
        "                        first few positions of each sample in the batch.\n",
        "        \"\"\"\n",
        "        # Clone the input tensor to avoid modifying the original\n",
        "        x_ = tf.identity(x)\n",
        "\n",
        "        #Ensure that the tensor is 4-dimensional (batch_size, channes, height, width)\n",
        "        assert len(x_.shape) == 4, \"Input tensor must have 4-dimensions.\"\n",
        "\n",
        "        #Zero out the specific area where the label will be overlaid\n",
        "        for idx, label in enumerate(y):\n",
        "            # If label has shape (batch_size, 1), extract the scalar\n",
        "            if len(label.shape) > 0:\n",
        "                label = int(label.numpy().flatten()[0])  # Extract scalar from Tensor safely\n",
        "\n",
        "            # Zero out the first few pixels of the first row (set to 0 across all channels)\n",
        "            x_ = tf.tensor_scatter_nd_update(\n",
        "                x_, [[idx, 0, i, c] for i in range(self.num_classes) for c in range(x_.shape[-1])],\n",
        "                tf.zeros([self.num_classes * x_.shape[-1]], dtype=x_.dtype)\n",
        "            )\n",
        "            # Set the label in the first row to 1.0 at the correct position\n",
        "            x_ = tf.tensor_scatter_nd_update(\n",
        "                x_, [[idx, 0, int(label), c] for c in range(x_.shape[-1])],\n",
        "                [1.0] * x_.shape[-1]  # Set all channels to 1.0\n",
        "            )\n",
        "\n",
        "        return x_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08bxpiiMSjNq"
      },
      "source": [
        "# Custom Fully Connected Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yznkBulfSjNq"
      },
      "outputs": [],
      "source": [
        "from sys import argv\n",
        "\n",
        "\n",
        "class FullyConnectedLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A fully connected (dense) layer that includes L2 normalization. ReLU activation and\n",
        "    custom training functionality.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = True, final_layer: bool = True):\n",
        "        \"\"\"\n",
        "        Initialize the fullyconnected layer.\n",
        "\n",
        "        Args:\n",
        "            in_features (int): The number of input features.\n",
        "            out_features (int): The number of output features.\n",
        "            final_layer (bool): Whether this is the final layer of the network. Deafult is False.\n",
        "            bias (bool): Whether to include a bias term in this network. Default is True.\n",
        "            fcl_threshold (float): Threshold value used in custom training function loss.\n",
        "            num_epochs (int): Number of epochs for custom training. Default is 100.\n",
        "            log_interval (int): Interval for logging loss during training. Default is 10.\n",
        "        \"\"\"\n",
        "\n",
        "        super(FullyConnectedLayer, self).__init__()\n",
        "        self.linear = layers.Dense(units = out_features, use_bias = bias)\n",
        "        self.relu = layers.ReLU()\n",
        "        self.opt = Adam(learning_rate = args.lr)\n",
        "        self.fcl_threshold = args.fcl_threshold\n",
        "        self.num_epochs = args.epochs\n",
        "        self.final_layer = final_layer\n",
        "\n",
        "    def forward(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Forward Pass through the layer including L2 normalization and ReLU activation.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor) : Input Tensor.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor : Output tensor after applying L2 normalization, linear transformation\n",
        "                        and ReLU activation.\n",
        "        \"\"\"\n",
        "        # L2 norm across the second axis (feature dimension)\n",
        "        x_direction = tf.linalg.l2_normalize(x, axis=1)\n",
        "        return self.relu(self.linear(x_direction))\n",
        "\n",
        "    def custom_train(self, x_pos: tf.Tensor, x_neg: tf.Tensor):\n",
        "        \"\"\"\n",
        "        Custom training loop for fully-connected layer using positive and negative samples.\n",
        "\n",
        "        Args:\n",
        "            x_pos (tf.Tensor) : Positive input data.\n",
        "            x_neg (tf.Tensor) : Negative input data.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[tf.Tensor, tf.Tensor] : Final Positive and Negative samples after training.\n",
        "        \"\"\"\n",
        "        for i in range(self.num_epochs):\n",
        "            print(f\"Models custom fully-connected layer training data on epoch {i}\")\n",
        "            with tf.GradientTape() as tape:\n",
        "                # calculate the goodness for both positive and negative samples\n",
        "                g_pos = tf.reduce_mean(tf.square(self.forward(x_pos))) # Mean over all dimensions\n",
        "                g_neg = tf.reduce_mean(tf.square(self.forward(x_neg))) # Mean over all dimensions\n",
        "\n",
        "                # calculate custom loss\n",
        "                loss_pos = -g_pos + self.fcl_threshold\n",
        "                loss_neg = g_neg - self.fcl_threshold\n",
        "                loss = tf.reduce_mean(tf.math.log(1 + tf.exp(tf.concat([loss_pos, loss_neg], axis = 0))))\n",
        "\n",
        "            # Apply Gradients\n",
        "            gradients = tape.gradient(loss, self.trainable_variables)\n",
        "            self.opt.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "            # logging at the specified interval\n",
        "            if i % self.log_interval == 0:\n",
        "                print(f\"Epoch {i}: Loss = {loss.numpy()}\")\n",
        "\n",
        "        #Return the forward pass outputs after training\n",
        "        return self.forward(x_pos), self.forward(x_neg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFSd26WcSjNr"
      },
      "source": [
        "# Custom Convolution Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3hI0rOJSjNr"
      },
      "outputs": [],
      "source": [
        "class Layer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A convolutional layer with L2 normalization, ReLU activation, and custom training functionality.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3, stride: int = 1,\n",
        "                 padding: str = 'valid', bias: bool = True, final_layer: bool = False,\n",
        "                 learning_rate: float = 0.001, conv_threshold: float = 1.0, num_epochs: int = 100,\n",
        "                 log_interval: int = 10):\n",
        "        \"\"\"\n",
        "        Initialize the Layer class.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): Number of input channels.\n",
        "            out_channels (int): Number of output channels.\n",
        "            kernel_size (int): Size of the convolution kernel. Default is 3.\n",
        "            stride (int): Stride for the convolution. Default is 1.\n",
        "            padding (str): Padding method ('valid' or 'same'). Default is 'valid'.\n",
        "            bias (bool): Whether to include a bias term in the convolution. Default is True.\n",
        "            final_layer (bool): Whether this is the final layer of the network. Default is False.\n",
        "            learning_rate (float): Learning rate for the optimizer. Default is 0.001.\n",
        "            conv_threshold (float): Threshold value used in the custom training loss function. Default is 1.0.\n",
        "            num_epochs (int): Number of epochs for custom training. Default is 100.\n",
        "            log_interval (int): Interval for logging and plotting during training. Default is 10.\n",
        "        \"\"\"\n",
        "        super(Layer, self).__init__()\n",
        "        self.conv = tf.keras.layers.Conv2D(filters=out_channels, kernel_size=kernel_size, strides=stride, padding=padding, use_bias=bias)\n",
        "        self.relu = layers.ReLU()\n",
        "        self.opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "        self.conv_threshold = conv_threshold\n",
        "        self.num_epochs = num_epochs\n",
        "        self.log_interval = log_interval\n",
        "        self.final_layer = final_layer\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass through the convolutional layer with L2 normalization and ReLU activation.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Output tensor after the forward pass.\n",
        "        \"\"\"\n",
        "        # L2 norm across the feature dimension\n",
        "        x_direction = tf.nn.l2_normalize(x, axis=1)\n",
        "        return self.relu(self.conv(x_direction))\n",
        "\n",
        "    def custom_train(self, x_pos: tf.Tensor, x_neg: tf.Tensor):\n",
        "        \"\"\"\n",
        "        Custom training loop for the convolutional layer using positive and negative samples.\n",
        "\n",
        "        Args:\n",
        "            x_pos (tf.Tensor): Positive input data.\n",
        "            x_neg (tf.Tensor): Negative input data.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[tf.Tensor, tf.Tensor]: Final positive and negative samples after training.\n",
        "        \"\"\"\n",
        "        # Initialize lists to hold loss and goodness values\n",
        "        loss_values = []\n",
        "        g_pos_values = []\n",
        "        g_neg_values = []\n",
        "\n",
        "        # Initialize the figure for plotting\n",
        "        fig = plt.figure(figsize=(12, 8))\n",
        "\n",
        "        for i in range(self.num_epochs):\n",
        "            print(f\"Model custom layer learning data on epoch {i}\")\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Forward pass for positive and negative samples\n",
        "                g_pos = tf.reduce_mean(tf.square(self.call(x_pos)), axis=[1, 2, 3])\n",
        "                g_neg = tf.reduce_mean(tf.square(self.call(x_neg)), axis=[1, 2, 3])\n",
        "\n",
        "                # Compute custom loss\n",
        "                loss_pos = -g_pos + self.conv_threshold\n",
        "                loss_neg = g_neg - self.conv_threshold\n",
        "                loss = tf.reduce_mean(tf.math.log(1 + tf.exp(tf.concat([loss_pos, loss_neg], axis=0))))\n",
        "\n",
        "            # Compute gradients and update weights\n",
        "            gradients = tape.gradient(loss, self.trainable_variables)\n",
        "            # if any(g is None for g in gradients):\n",
        "            #     print(f\"Layer {self}: Gradients contain None values!\")\n",
        "\n",
        "            # Apply gradients only if they are valid and there are trainable variables\n",
        "            # if self.trainable_variables:\n",
        "            self.opt.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "            # else:\n",
        "                # print(f\"No trainable variables for layer, skipping gradient application.\")\n",
        "\n",
        "            # Log and plot loss and goodness values at intervals\n",
        "            if i % self.log_interval == 0:\n",
        "                loss_values.append(loss.numpy())\n",
        "                g_pos_values.append(tf.reduce_mean(g_pos).numpy())\n",
        "                g_neg_values.append(tf.reduce_mean(g_neg).numpy())\n",
        "\n",
        "                # Plotting\n",
        "                plt.subplot(3, 1, 1)\n",
        "                plt.plot(loss_values, color='blue')\n",
        "                plt.title(\"Loss during training\")\n",
        "\n",
        "                plt.subplot(3, 1, 2)\n",
        "                plt.plot(g_pos_values, color='green')\n",
        "                plt.title(\"g_pos during training\")\n",
        "\n",
        "                plt.subplot(3, 1, 3)\n",
        "                plt.plot(g_neg_values, color='red')\n",
        "                plt.title(\"g_neg during training\")\n",
        "\n",
        "                plt.tight_layout()\n",
        "                clear_output(wait=True)  # Clear the output to update the plot\n",
        "                plt.show()\n",
        "\n",
        "            # Print the loss at each step\n",
        "            print(f'Loss at step {i}: {loss.numpy()}')\n",
        "\n",
        "        # Return the final outputs after training\n",
        "        return self.call(x_pos), self.call(x_neg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxRnEWHtSjNs"
      },
      "source": [
        "## Custom Network Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq2LV6rHSjNs"
      },
      "outputs": [],
      "source": [
        "class Net(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    A neural network model that includes multiple convolutional layers, ReLU activations,\n",
        "    and custom training functionality.\n",
        "    Inherits from `tf.keras.Model`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes: int = 10):\n",
        "        \"\"\"\n",
        "        Initialize the Net model with convolutional layers, fully connected layers,\n",
        "        and ReLU activations.\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): Number of output classes. Default is 10.\n",
        "        \"\"\"\n",
        "        super(Net, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Instantiate the LabelManipulator\n",
        "        self.label_manipulator = LabelManipulator(num_classes=num_classes)\n",
        "\n",
        "        # Define the layers\n",
        "        self.layers_ = [\n",
        "            # Block 1\n",
        "            Layer(3, 64, kernel_size=3, padding='same'),  # Conv 1\n",
        "            tf.keras.layers.ReLU(),\n",
        "            Layer(64, 64, kernel_size=3, padding='same'),  # Conv 2\n",
        "            tf.keras.layers.ReLU(),\n",
        "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),  # MaxPooling\n",
        "\n",
        "            # Block 2\n",
        "            Layer(64, 128, kernel_size=3, padding='same'),  # Conv 3\n",
        "            tf.keras.layers.ReLU(),\n",
        "            Layer(128, 128, kernel_size=3, padding='same'),  # Conv 4\n",
        "            tf.keras.layers.ReLU(),\n",
        "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),  # MaxPooling\n",
        "\n",
        "            # Block 3\n",
        "            Layer(128, 256, kernel_size=3, padding='same'),  # Conv 5\n",
        "            tf.keras.layers.ReLU(),\n",
        "            Layer(256, 256, kernel_size=3, padding='same'),  # Conv 6\n",
        "            tf.keras.layers.ReLU(),\n",
        "            Layer(256, 256, kernel_size=3, padding='same'),  # Conv 7\n",
        "            tf.keras.layers.ReLU(),\n",
        "            Layer(256, 256, kernel_size=3, padding='same'),  # Conv 8\n",
        "            tf.keras.layers.ReLU(),\n",
        "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),  # MaxPooling\n",
        "\n",
        "            # Block 4\n",
        "            Layer(256, 512, kernel_size=3, padding='same'),  # Conv 9\n",
        "            tf.keras.layers.ReLU(),\n",
        "            Layer(512, 512, kernel_size=3, padding='same'),  # Conv 10\n",
        "            tf.keras.layers.ReLU(),\n",
        "            Layer(512, 512, kernel_size=3, padding='same'),  # Conv 11\n",
        "            tf.keras.layers.ReLU(),\n",
        "            Layer(512, 512, kernel_size=3, padding='same'),  # Conv 12\n",
        "            tf.keras.layers.ReLU(),\n",
        "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),  # MaxPooling\n",
        "\n",
        "            # Block 5\n",
        "            Layer(512, 512, kernel_size=3, padding='same'),  # Conv 13\n",
        "            tf.keras.layers.ReLU(),\n",
        "            Layer(512, 512, kernel_size=3, padding='same'),  # Conv 14\n",
        "            tf.keras.layers.ReLU(),\n",
        "            Layer(512, 512, kernel_size=3, padding='same'),  # Conv 15\n",
        "            tf.keras.layers.ReLU(),\n",
        "            Layer(512, 512, kernel_size=3, padding='same'),  # Conv 16\n",
        "            tf.keras.layers.ReLU(),\n",
        "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),  # MaxPooling\n",
        "\n",
        "            # Fully Connected Layers\n",
        "            tf.keras.layers.Flatten(),  # Flatten the input for Dense layers\n",
        "            FullyConnectedLayer(512 * 7 * 7, 4096),  # Dense 1 (4096 units)\n",
        "            tf.keras.layers.ReLU(),\n",
        "            FullyConnectedLayer(4096, 4096),  # Dense 2 (4096 units)\n",
        "            tf.keras.layers.ReLU(),\n",
        "            FullyConnectedLayer(4096, 10, final_layer=True),  # Output layer (10 classes for CIFAR-10)\n",
        "        ]\n",
        "\n",
        "      #[\n",
        "        #     Layer(3, 64, kernel_size=5, padding='same'),  # Layer 0\n",
        "        #     tf.keras.layers.ReLU(),\n",
        "\n",
        "        #     Layer(64, 128, kernel_size=5, padding='same', stride=2),  # Layer 2\n",
        "        #     tf.keras.layers.ReLU(),\n",
        "\n",
        "        #     Layer(128, 256, kernel_size=5, padding='same', stride=2),  # Layer 4\n",
        "        #     tf.keras.layers.ReLU(),\n",
        "\n",
        "        #     Layer(256, 512, kernel_size=5, padding='same', stride=2),  # Layer 6\n",
        "        #     tf.keras.layers.ReLU(),\n",
        "\n",
        "        #     Layer(512, 1024, kernel_size=5, padding='same', stride=2),  # Layer 8\n",
        "        #     tf.keras.layers.ReLU(),\n",
        "\n",
        "        #     Layer(1024, 2048, kernel_size=5, padding='same', stride=2),  # Layer 10\n",
        "        #     tf.keras.layers.ReLU(),\n",
        "\n",
        "        #     FullyConnectedLayer(2048, 10, final_layer=True),  # Layer 12\n",
        "        # ]\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Output tensor after passing through all layers.\n",
        "        \"\"\"\n",
        "        for layer in self.layers_:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def predict(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Make predictions by overlaying labels and calculating goodness for each label.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Index of the label with the highest goodness score.\n",
        "        \"\"\"\n",
        "        goodness_per_label = []\n",
        "        for label in range(self.num_classes):\n",
        "            # Use the overlay_y_on_x method from the LabelManipulator class\n",
        "            h = self.label_manipulator.overlay_y_on_x(x, tf.constant([label]))\n",
        "            goodness = []\n",
        "            for layer in self.layers_:\n",
        "                h = layer(h)\n",
        "                goodness.append(tf.reduce_sum(tf.square(h)) / tf.size(h, out_type=tf.float32))\n",
        "            goodness_per_label.append(tf.reduce_sum(tf.stack(goodness)))\n",
        "\n",
        "        return tf.argmax(tf.stack(goodness_per_label))\n",
        "\n",
        "    def custom_train(self, x_pos: tf.Tensor, x_neg: tf.Tensor):\n",
        "        \"\"\"\n",
        "        Custom training logic, training each layer separately if it belongs to specific classes.\n",
        "\n",
        "        Args:\n",
        "            x_pos (tf.Tensor): Positive input data.\n",
        "            x_neg (tf.Tensor): Negative input data.\n",
        "        \"\"\"\n",
        "        h_pos, h_neg = x_pos, x_neg\n",
        "        for i, layer in enumerate(self.layers_):\n",
        "            print(f\"Layer {i} trainable variables: {layer.trainable_variables}\")\n",
        "            print(f\"Training layer: {i}\")\n",
        "            if isinstance(layer, Layer):  # Call custom_train on instances of the Layer class\n",
        "                h_pos, h_neg = layer.custom_train(h_pos, h_neg)\n",
        "            elif isinstance(layer, FullyConnectedLayer):  # Call custom_train on instances of FullyConnectedLayer\n",
        "                h_pos, h_neg = layer.custom_train(h_pos, h_neg)\n",
        "            else:  # For other layers, just pass the data through\n",
        "                h_pos = layer(h_pos)\n",
        "                h_neg = layer(h_neg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPmit00tSjNs"
      },
      "source": [
        "# Network hyperparameter setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7MkGuj-SjNt"
      },
      "outputs": [],
      "source": [
        "# class Args:\n",
        "#     \"\"\"\n",
        "#     A class to store hyperparameters and configuration settings.\n",
        "#     \"\"\"\n",
        "#     train_size = 1000  # Default training size\n",
        "#     test_size = 100  # Default testing size\n",
        "#     epochs = 1000  # Number of epochs for training\n",
        "#     lr = 0.05  # Learning rate\n",
        "#     no_cuda = False  # Whether to disable CUDA (GPU acceleration)\n",
        "#     no_mps = False  # Whether to disable MPS (Apple's GPU acceleration)\n",
        "#     save_model = False  # Whether to save the trained model\n",
        "#     fcl_threshold = 1  # Threshold for FullyConnectedLayer\n",
        "#     conv_threshold = 0.02  # Threshold for convolutional layers\n",
        "#     seed = 1234  # Random seed for reproducibility\n",
        "#     log_interval = 10  # How often to log training information\n",
        "\n",
        "class Args:\n",
        "    \"\"\"\n",
        "    A class to store hyperparameters and configuration settings.\n",
        "    \"\"\"\n",
        "    train_size = 50000  # Full CIFAR-10 training set size\n",
        "    test_size = 10000  # Full CIFAR-10 test set size\n",
        "    epochs = 200  # Number of epochs for training\n",
        "    lr = 0.001  # Learning rate for stable training (adjust if needed)\n",
        "    no_cuda = False  # Whether to disable CUDA (GPU acceleration)\n",
        "    no_mps = False  # Whether to disable MPS (Apple's GPU acceleration)\n",
        "    save_model = True  # Whether to save the trained model\n",
        "    fcl_threshold = 1  # Threshold for FullyConnectedLayer\n",
        "    conv_threshold = 0.02  # Threshold for convolutional layers\n",
        "    seed = 1234  # Random seed for reproducibility\n",
        "    log_interval = 10  # Log progress every 10 batches\n",
        "\n",
        "# Create an instance of the Args class\n",
        "args = Args()\n",
        "\n",
        "# Set the random seed for TensorFlow\n",
        "tf.random.set_seed(args.seed)\n",
        "\n",
        "# Determine whether to use GPU or CPU\n",
        "use_cuda = not args.no_cuda and tf.config.list_physical_devices('GPU')\n",
        "use_mps = not args.no_mps and tf.config.list_physical_devices('MPS')\n",
        "\n",
        "if use_cuda:\n",
        "    device = \"GPU\"\n",
        "elif use_mps:\n",
        "    device = \"MPS\"\n",
        "else:\n",
        "    device = \"CPU\"\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set batch size configurations\n",
        "train_kwargs = {\"batch_size\": args.train_size}\n",
        "test_kwargs = {\"batch_size\": args.test_size}\n",
        "\n",
        "# If GPU (or MPS) is available, configure additional options for performance optimization\n",
        "if use_cuda or use_mps:\n",
        "    additional_kwargs = {\"shuffle\": True}\n",
        "    train_kwargs.update(additional_kwargs)\n",
        "    test_kwargs.update(additional_kwargs)\n",
        "\n",
        "# Example use of the configurations\n",
        "print(f\"Training configuration: {train_kwargs}\")\n",
        "print(f\"Testing configuration: {test_kwargs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7_B9zsZSjNt"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msVZSDESSjNu"
      },
      "outputs": [],
      "source": [
        "def normalize_images(image, mean, std):\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # Convert to float32 and scale to [0, 1]\n",
        "    image = (image - mean) / std  # Normalize\n",
        "    return image\n",
        "\n",
        "\n",
        "# Load CIFAR-10 data using TensorFlow\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "# Normalize mean and std\n",
        "mean = tf.constant([0.4914, 0.4822, 0.4465])\n",
        "std = tf.constant([0.2023, 0.1994, 0.2010])\n",
        "\n",
        "# Data preprocessing: Normalize the images and convert labels to one-hot encoding if needed\n",
        "train_images = normalize_images(train_images, mean, std)\n",
        "test_images = normalize_images(test_images, mean, std)\n",
        "\n",
        "# Optional: One-hot encode labels if needed (comment this out if not required)\n",
        "# train_labels = to_categorical(train_labels, num_classes=10)\n",
        "# test_labels = to_categorical(test_labels, num_classes=10)\n",
        "\n",
        "# Define TensorFlow datasets for training and testing\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "\n",
        "# Batch the datasets and apply any additional settings from config\n",
        "train_dataset = train_dataset.shuffle(buffer_size=50000).batch(64)     #.batch(args.train_size)  # Shuffle and batch the training data\n",
        "test_dataset = test_dataset.batch(64)     #.batch(args.test_size)  # Only batch the testing data\n",
        "\n",
        "# Prefetching to improve performance\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJkn8QQLSjNu"
      },
      "source": [
        "# Initialize and train network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hgnlYdxSjNu"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model and move it to the correct device (if GPU or MPS is available)\n",
        "net = Net()\n",
        "device = \"GPU\" if tf.config.list_physical_devices('GPU') else \"CPU\"\n",
        "print(f\"Model will run on: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TvC5NB7SjNu"
      },
      "outputs": [],
      "source": [
        "# Load a batch of data (using TensorFlow dataset)\n",
        "for x, y in train_dataset.take(1):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj7IE8BHSjNv"
      },
      "outputs": [],
      "source": [
        "# Overlay the labels on the input data\n",
        "label_manipulator = LabelManipulator(num_classes=10)\n",
        "x_pos = label_manipulator.overlay_y_on_x(x, y)\n",
        "y_neg = label_manipulator.get_y_neg(y)\n",
        "x_neg = label_manipulator.overlay_y_on_x(x, y_neg)\n",
        "\n",
        "# Inspect the tensor shapes\n",
        "print(f\"x shape: {x.shape}\")\n",
        "print(f\"Batch size: {x.shape[0]}\")\n",
        "print(f\"y shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0lFYZo4SjNv"
      },
      "outputs": [],
      "source": [
        "# Visualize samples\n",
        "fig, axs = plt.subplots(5, 3, figsize=(10, 10))\n",
        "\n",
        "# Define a dictionary to map class indices to class names\n",
        "class_dict = {i: 'class_' + str(i) for i in range(10)}\n",
        "\n",
        "for i in range(5):\n",
        "    # Convert TensorFlow tensors to numpy arrays and transpose for correct plotting\n",
        "    img = x[i].numpy()\n",
        "    pos_img = x_pos[i].numpy()\n",
        "    neg_img = x_neg[i].numpy()\n",
        "\n",
        "    # Display original image\n",
        "    axs[i, 0].imshow(img, interpolation = 'nearest')\n",
        "    axs[i, 0].set_title('Original: ' + class_dict[int(y[i])] + '\\n Shape: ' + str(img.shape))\n",
        "\n",
        "    # Display positive (overlaid label) image\n",
        "    axs[i, 1].imshow(pos_img, interpolation = 'nearest')\n",
        "    axs[i, 1].set_title('Positive: ' + class_dict[int(y[i])] + '\\n Shape: ' + str(pos_img.shape))\n",
        "\n",
        "    # Display negative image\n",
        "    axs[i, 2].imshow(neg_img, interpolation = 'nearest')\n",
        "    axs[i, 2].set_title('Negative: ' + class_dict[int(y_neg[i])] + '\\n Shape: ' + str(neg_img.shape))\n",
        "\n",
        "# Remove axis ticks\n",
        "for ax in axs.flat:\n",
        "    ax.axis('off')\n",
        "\n",
        "# Adjust layout for better visualization\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1L3pRJESjNv"
      },
      "source": [
        "# Train the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q2kjAHMSjNw"
      },
      "outputs": [],
      "source": [
        "net.custom_train(x_pos, x_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqQzoLnpSjNw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}