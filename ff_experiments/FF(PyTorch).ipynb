{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvhOGGsVDVmr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates negative labels for the training data, which are required for contrastive divergence training\n",
        "def get_y_neg(y):\n",
        "    y_neg = y.clone()\n",
        "    for idx, y_samp in enumerate(y):\n",
        "        allowed_indices = list(range(10))\n",
        "        allowed_indices.remove(y_samp.item())\n",
        "        y_neg[idx] = torch.tensor(allowed_indices)[torch.randint(len(allowed_indices), size=(1,))].item()\n",
        "    return y_neg.to(device)"
      ],
      "metadata": {
        "id": "2ddjzK6JDdnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def overlay_y_on_x(x, y, classes=10):\n",
        "    x_ = x.clone()\n",
        "    x_[range(x.shape[0]), :, 0, :classes] *= 0.0\n",
        "    x_[range(x.shape[0]), :, 0, y] = 1\n",
        "    return x_"
      ],
      "metadata": {
        "id": "Ov2UJLMCDhLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Net class that inherits from torch.nn.Module which is the base class for all neural network modules in PyTorch.\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            Layer(3, 64, kernel_size=5, padding=2),  # Layer 0\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            Layer(64, 128, kernel_size=5, padding=2, stride=2),  # Layer 2\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            Layer(128, 256, kernel_size=5, padding=2, stride=2),  # Layer 4\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            Layer(256, 512, kernel_size=5, padding=2, stride=2),  # Layer 6\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            Layer(512, 1024, kernel_size=5, padding=2, stride=2),  # Layer 8\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            Layer(1024, 2048, kernel_size=5, padding=2, stride=2),  # Layer 10\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            Layer(2048, 10, kernel_size=1, stride=1),  # Layer 12\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "    def predict(self, x):\n",
        "        goodness_per_label = []\n",
        "        for label in range(10):\n",
        "            h = overlay_y_on_x(x, label)\n",
        "            goodness = []\n",
        "            for layer in self.layers:\n",
        "                h = layer(h)\n",
        "                goodness += [(h.pow(2).sum() / h.numel()).unsqueeze(0)]\n",
        "            goodness_per_label += [torch.sum(torch.stack(goodness)).unsqueeze(0)]\n",
        "        goodness_per_label = torch.cat(goodness_per_label, 0)\n",
        "        return goodness_per_label.argmax(0)\n",
        "\n",
        "    def custom_train(self, x_pos, x_neg):\n",
        "        h_pos, h_neg = x_pos, x_neg\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            print(\"training layer: \", i)\n",
        "            if isinstance(layer, Layer):  # only call custom_train on instances of the Layer class\n",
        "                h_pos, h_neg = layer.custom_train(h_pos, h_neg)\n",
        "            elif isinstance(layer, FullyConnectedLayer):  # only call custom_train on instances of the FullyConnectedLayer class\n",
        "                h_pos, h_neg = layer.custom_train(h_pos, h_neg)\n",
        "            else:  # for other layers, just pass the data through\n",
        "                h_pos = layer(h_pos)\n",
        "                h_neg = layer(h_neg)"
      ],
      "metadata": {
        "id": "sUXcT79qDlqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fully Connected layers of the network.\n",
        "class FullyConnectedLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True, final_layer=False):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.opt = Adam(self.parameters(), lr=args.lr)\n",
        "        self.fcl_threshold = args.fcl_threshold\n",
        "        self.num_epochs = args.epochs\n",
        "        self.final_layer = final_layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        #L2 norm\n",
        "        x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)\n",
        "        return self.relu(self.linear(x_direction))\n",
        "\n",
        "    def custom_train(self, x_pos, x_neg):\n",
        "        for i in range(self.num_epochs):\n",
        "            g_pos = self.forward(x_pos).pow(2).mean().unsqueeze(0)  # mean over all dimensions in a sample\n",
        "            g_neg = self.forward(x_neg).pow(2).mean().unsqueeze(0)  # mean over all dimensions in a sample\n",
        "            loss = torch.log(1 + torch.exp(torch.cat([-g_pos + self.fcl_threshold, g_neg - self.fcl_threshold]))).mean()\n",
        "            self.opt.zero_grad()\n",
        "            loss.backward()\n",
        "            self.opt.step()\n",
        "            if i % args.log_interval == 0:\n",
        "                print(\"Loss: \", loss.item())\n",
        "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()"
      ],
      "metadata": {
        "id": "En8YRiz8D84D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolutional layers of the network.\n",
        "class Layer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0, bias=True, final_layer=False):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.opt = Adam(self.parameters(), lr=args.lr)\n",
        "        self.conv_threshold = args.conv_threshold\n",
        "        self.num_epochs = args.epochs\n",
        "        self.final_layer = final_layer\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        '''\n",
        "        # trial 10\n",
        "        # batch norm\n",
        "        x = self.conv(x)\n",
        "        return self.relu(x)\n",
        "        '''\n",
        "\n",
        "        # L2 norm\n",
        "        x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)\n",
        "        return self.relu(self.conv(x_direction))\n",
        "\n",
        "    def custom_train(self, x_pos, x_neg):\n",
        "\n",
        "        # initialize lists to hold values\n",
        "        loss_values = []\n",
        "        g_pos_values = []\n",
        "        g_neg_values = []\n",
        "\n",
        "        # initialize figure\n",
        "        fig = plt.figure(figsize=(12,8))\n",
        "\n",
        "        for i in range(self.num_epochs):\n",
        "            g_pos = self.forward(x_pos).pow(2).mean(dim=(1,2,3)).unsqueeze(0)  # mean over all dimensions\n",
        "            g_neg = self.forward(x_neg).pow(2).mean(dim=(1,2,3)).unsqueeze(0)  # mean over all dimensions\n",
        "            loss = torch.log(1 + torch.exp(torch.cat([-g_pos + self.conv_threshold, g_neg - self.conv_threshold]))).mean()\n",
        "            self.opt.zero_grad()\n",
        "\n",
        "            #loss.backward()\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "            self.opt.step()\n",
        "\n",
        "            if i % args.log_interval == 0:\n",
        "                loss_values.append(loss.item())\n",
        "                g_pos_values.append(g_pos.mean().item())  # take mean of all batch values\n",
        "                g_neg_values.append(g_neg.mean().item())  # take mean of all batch values\n",
        "\n",
        "                # plotting\n",
        "                plt.subplot(3,1,1)\n",
        "                plt.plot(loss_values, color='blue')\n",
        "                plt.title(\"Loss during training\")\n",
        "\n",
        "                plt.subplot(3,1,2)\n",
        "                plt.plot(g_pos_values, color='green')\n",
        "                plt.title(\"g_pos during training\")\n",
        "\n",
        "                plt.subplot(3,1,3)\n",
        "                plt.plot(g_neg_values, color='red')\n",
        "                plt.title(\"g_neg during training\")\n",
        "\n",
        "                plt.tight_layout()\n",
        "                clear_output(wait=True)  # this clears the output of the cell, useful for updating the plots\n",
        "                plt.show()\n",
        "\n",
        "            # Print the loss at each step\n",
        "            print(f'Loss at step {i}: {loss.item()}')\n",
        "\n",
        "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()"
      ],
      "metadata": {
        "id": "Tl-jaoYBD9r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    train_size = 1000 #10000 #50000\n",
        "    test_size = 100 #2000 #10000\n",
        "    epochs = 1000\n",
        "    lr = 0.05\n",
        "    no_cuda = False\n",
        "    no_mps = False\n",
        "    save_model = False\n",
        "    fcl_threshold = 1\n",
        "    conv_threshold = 0.02\n",
        "    seed = 1234\n",
        "    log_interval = 10\n",
        "\n",
        "args = Args()\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
        "if use_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "elif use_mps:\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "train_kwargs = {\"batch_size\": args.train_size}\n",
        "test_kwargs = {\"batch_size\": args.test_size}\n",
        "if use_cuda:\n",
        "    cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)"
      ],
      "metadata": {
        "id": "ZFW2XLR4ED7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "transform = Compose(\n",
        "    [\n",
        "        ToTensor(),\n",
        "        Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(CIFAR10(\"./data/\", train=True, download=True, transform=transform), **train_kwargs)\n",
        "test_loader = DataLoader(CIFAR10(\"./data/\", train=False, download=True, transform=transform), **test_kwargs)"
      ],
      "metadata": {
        "id": "ZbnUVt5UEHMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Model\n",
        "net = Net().to(device)"
      ],
      "metadata": {
        "id": "rgtoubPjEKv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.onnx\n",
        "\n",
        "# Assuming `net` is your PyTorch model and it's already built\n",
        "dummy_input = torch.randn(1, 3, 128, 128)  # Adjust input shape as per your model requirements\n",
        "onnx_file_path = \"model.onnx\"\n",
        "\n",
        "# Export the model to ONNX format\n",
        "torch.onnx.export(\n",
        "    net,                    # The PyTorch model\n",
        "    dummy_input,            # A dummy input tensor for tracing\n",
        "    onnx_file_path,         # File to save the ONNX model\n",
        "    export_params=True,     # Store the trained parameter weights inside the model file\n",
        "    opset_version=11,       # ONNX opset version (11 is commonly used)\n",
        "    do_constant_folding=True,  # Perform constant folding for optimization\n",
        "    input_names=[\"input\"],  # Input tensor name\n",
        "    output_names=[\"output\"],  # Output tensor name\n",
        "    dynamic_axes={\n",
        "        \"input\": {0: \"batch_size\"},  # Allow dynamic batch size\n",
        "        \"output\": {0: \"batch_size\"}\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Model has been exported to {onnx_file_path}\")\n"
      ],
      "metadata": {
        "id": "AZK3-k554Swi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchview import draw_graph\n",
        "\n",
        "model_graph = draw_graph(net, input_size=(1,3,32,32),depth=3, expand_nested=False)\n",
        "\n",
        "model_graph.resize_graph\n",
        "model_graph.visual_graph\n"
      ],
      "metadata": {
        "id": "9D6R51jU0R4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "import torch\n",
        "\n",
        "# Create a dummy input to pass through the network\n",
        "dummy_input = torch.randn(1, 3, 32, 32)  # Adjust input shape as per your data\n",
        "output = net(dummy_input)\n",
        "dot = make_dot(output, params=dict(net.named_parameters()), show_attrs=False, show_saved=False)\n",
        "dot.render(\"network_architecture\", format=\"png\")"
      ],
      "metadata": {
        "id": "Ewb6gjn1y378"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = next(iter(train_loader))\n",
        "x, y = x.to(device), y.to(device)\n",
        "x_pos = overlay_y_on_x(x, y)\n",
        "y_neg = get_y_neg(y)\n",
        "x_neg = overlay_y_on_x(x, y_neg)"
      ],
      "metadata": {
        "id": "YGAgJIf1ENlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.size()"
      ],
      "metadata": {
        "id": "TorV2wUKEQfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.size(0)"
      ],
      "metadata": {
        "id": "zaOJBwA4ES33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.size()"
      ],
      "metadata": {
        "id": "ezs8INvhET-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(5, 3, figsize=(10, 10))\n",
        "\n",
        "# Define a dictionary to map class indices to class names (replace this with your actual classes)\n",
        "class_dict = {i: 'class_' + str(i) for i in range(10)}\n",
        "\n",
        "for i in range(5):\n",
        "    img = x[i].cpu().numpy().transpose(1,2,0)\n",
        "    pos_img = x_pos[i].cpu().numpy().transpose(1,2,0)\n",
        "    neg_img = x_neg[i].cpu().numpy().transpose(1,2,0)\n",
        "\n",
        "    axs[i, 0].imshow(img)\n",
        "    axs[i, 0].set_title('Original: ' + class_dict[int(y[i])] + '\\n Shape: ' + str(img.shape))\n",
        "\n",
        "    axs[i, 1].imshow(pos_img)\n",
        "    axs[i, 1].set_title('Positive: ' + class_dict[int(y[i])] + '\\n Shape: ' + str(pos_img.shape))\n",
        "\n",
        "    axs[i, 2].imshow(neg_img)\n",
        "    axs[i, 2].set_title('Negative: ' + class_dict[int(y_neg[i])] + '\\n Shape: ' + str(neg_img.shape))\n",
        "\n",
        "for ax in axs.flat:\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()  # Adjusts subplot params so that subplots are nicely fit in the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1bmrdqGgEWX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.custom_train(x_pos, x_neg)"
      ],
      "metadata": {
        "id": "sHx0SPtzEZg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train Accuracy: {:.2f}%\".format(100 * net.predict(x).eq(y).float().mean().item()))"
      ],
      "metadata": {
        "id": "PrRiukayEcYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Model\n",
        "x_te, y_te = next(iter(test_loader))\n",
        "x_te, y_te = x_te.to(device), y_te.to(device)\n",
        "if args.save_model:\n",
        "    torch.save(net.state_dict(), \"cifar10_ff.pt\")"
      ],
      "metadata": {
        "id": "FTntUGRiEieI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test Accuracy: {:.2f}%\".format(100 * net.predict(x_te).eq(y_te).float().mean().item()))"
      ],
      "metadata": {
        "id": "sURU3wknEklr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mziOKKQJUo_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bb_ZwIWzUo7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4boH75avUo4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8cOCeYgQUo1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-_qTrPFUoo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "from torchvision.datasets import MNIST, CIFAR10\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def MNIST_loaders(train_batch_size=50000, test_batch_size=10000):\n",
        "\n",
        "    transform = Compose([\n",
        "        ToTensor(),\n",
        "        Normalize((0.1307,), (0.3081,)),\n",
        "        Lambda(lambda x: torch.flatten(x))])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        MNIST('./data/', train=True,\n",
        "              download=True,\n",
        "              transform=transform),\n",
        "        batch_size=train_batch_size, shuffle=True)\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        MNIST('./data/', train=False,\n",
        "              download=True,\n",
        "              transform=transform),\n",
        "        batch_size=test_batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def overlay_y_on_x(x, y):\n",
        "    \"\"\"Replace the first 10 pixels of data [x] with one-hot-encoded label [y]\n",
        "    \"\"\"\n",
        "    x_ = x.clone()\n",
        "    x_[:, :10] *= 0.0\n",
        "    x_[range(x.shape[0]), y] = x.max()\n",
        "    return x_\n",
        "\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, dims):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        for d in range(len(dims) - 1):\n",
        "            self.layers += [Layer(dims[d], dims[d + 1]).cuda()]\n",
        "\n",
        "    def predict(self, x):\n",
        "        goodness_per_label = []\n",
        "        for label in range(10):\n",
        "            h = overlay_y_on_x(x, label)\n",
        "            goodness = []\n",
        "            for layer in self.layers:\n",
        "                h = layer(h)\n",
        "                goodness += [h.pow(2).mean(1)]\n",
        "            goodness_per_label += [sum(goodness).unsqueeze(1)]\n",
        "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
        "        return goodness_per_label.argmax(1)\n",
        "\n",
        "    def train(self, x_pos, x_neg):\n",
        "        h_pos, h_neg = x_pos, x_neg\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            print('training layer', i, '...')\n",
        "            h_pos, h_neg = layer.train(h_pos, h_neg)\n",
        "\n",
        "\n",
        "class Layer(nn.Linear):\n",
        "    def __init__(self, in_features, out_features,\n",
        "                 bias=True, device=None, dtype=None):\n",
        "        super().__init__(in_features, out_features, bias, device, dtype)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.opt = Adam(self.parameters(), lr=0.03)\n",
        "        self.threshold = 2.0\n",
        "        self.num_epochs = 1000\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)\n",
        "        return self.relu(\n",
        "            torch.mm(x_direction, self.weight.T) +\n",
        "            self.bias.unsqueeze(0))\n",
        "\n",
        "    def train(self, x_pos, x_neg):\n",
        "        for i in tqdm(range(self.num_epochs)):\n",
        "            g_pos = self.forward(x_pos).pow(2).mean(1)\n",
        "            g_neg = self.forward(x_neg).pow(2).mean(1)\n",
        "            # The following loss pushes pos (neg) samples to\n",
        "            # values larger (smaller) than the self.threshold.\n",
        "            loss = torch.log(1 + torch.exp(torch.cat([\n",
        "                -g_pos + self.threshold,\n",
        "                g_neg - self.threshold]))).mean()\n",
        "            self.opt.zero_grad()\n",
        "            # this backward just compute the derivative and hence\n",
        "            # is not considered backpropagation.\n",
        "            loss.backward()\n",
        "            self.opt.step()\n",
        "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()\n",
        "\n",
        "\n",
        "def visualize_sample(data, name='', idx=0):\n",
        "    reshaped = data[idx].cpu().reshape(28, 28)\n",
        "    plt.figure(figsize = (4, 4))\n",
        "    plt.title(name)\n",
        "    plt.imshow(reshaped, cmap=\"gray\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(1234)\n",
        "    train_loader, test_loader = MNIST_loaders()\n",
        "\n",
        "    net = Net([784, 500, 500])\n",
        "    x, y = next(iter(train_loader))\n",
        "    x, y = x.cuda(), y.cuda()\n",
        "    x_pos = overlay_y_on_x(x, y)\n",
        "    rnd = torch.randperm(x.size(0))\n",
        "    x_neg = overlay_y_on_x(x, y[rnd])\n",
        "\n",
        "    for data, name in zip([x, x_pos, x_neg], ['orig', 'pos', 'neg']):\n",
        "        visualize_sample(data, name)\n",
        "\n",
        "    net.train(x_pos, x_neg)\n",
        "\n",
        "    print('train error:', 1.0 - net.predict(x).eq(y).float().mean().item())\n",
        "\n",
        "    x_te, y_te = next(iter(test_loader))\n",
        "    x_te, y_te = x_te.cuda(), y_te.cuda()\n",
        "\n",
        "    print('test error:', 1.0 - net.predict(x_te).eq(y_te).float().mean().item())"
      ],
      "metadata": {
        "id": "-vJLQ1PKUolf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def CIFAR10_loaders(train_batch_size=5000, test_batch_size=1000):\n",
        "    transform = Compose([\n",
        "        ToTensor(),\n",
        "        Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # CIFAR-10 normalization\n",
        "    ])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        CIFAR10('./data/', train=True, download=True, transform=transform),\n",
        "        batch_size=train_batch_size, shuffle=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        CIFAR10('./data/', train=False, download=True, transform=transform),\n",
        "        batch_size=test_batch_size, shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "def overlay_y_on_x(x, y, num_classes=10):\n",
        "    \"\"\"\n",
        "    Overlay one-hot encoded labels onto the images.\n",
        "    Note: CIFAR-10 images have shape [batch, channels, height, width].\n",
        "    \"\"\"\n",
        "    x_ = x.clone()\n",
        "    x_[:, :, :num_classes, 0] = 0  # Set the first few pixels to zero\n",
        "    x_[range(x.shape[0]), :, y, 0] = x.max()  # Set pixel based on label\n",
        "    return x_\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([Layer(dims[i], dims[i+1]).to(device) for i in range(len(dims)-1)])\n",
        "\n",
        "    def predict(self, x):\n",
        "        goodness_per_label = []\n",
        "        for label in range(10):\n",
        "            h = overlay_y_on_x(x, label)\n",
        "            goodness = []\n",
        "            for layer in self.layers:\n",
        "                h = layer(h)\n",
        "                goodness.append(h.pow(2).mean((1, 2, 3)))  # Adjust mean for all spatial dims\n",
        "            goodness_per_label.append(torch.stack(goodness).sum(0).unsqueeze(1))\n",
        "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
        "        return goodness_per_label.argmax(1)\n",
        "\n",
        "    def train(self, x_pos, x_neg):\n",
        "        h_pos, h_neg = x_pos, x_neg\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            print('training layer', i, '...')\n",
        "            h_pos, h_neg = layer.train(h_pos, h_neg)\n",
        "\n",
        "class Layer(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_features, out_features, kernel_size=3, stride=1, padding=1).to(device)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.opt = Adam(self.parameters(), lr=0.03)\n",
        "        self.threshold = 2.0\n",
        "        self.num_epochs = 1000\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)  # L2 normalization\n",
        "        return self.relu(self.conv(x_direction))\n",
        "\n",
        "    def train(self, x_pos, x_neg):\n",
        "        for i in tqdm(range(self.num_epochs)):\n",
        "            g_pos = self.forward(x_pos).pow(2).mean((1, 2, 3))  # Mean over spatial dimensions\n",
        "            g_neg = self.forward(x_neg).pow(2).mean((1, 2, 3))\n",
        "            # Loss that pushes pos (neg) samples to values above (below) threshold\n",
        "            loss = torch.log(1 + torch.exp(torch.cat([-g_pos + self.threshold, g_neg - self.threshold]))).mean()\n",
        "            self.opt.zero_grad()\n",
        "            loss.backward()\n",
        "            self.opt.step()\n",
        "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()\n",
        "\n",
        "def visualize_sample(data, name='', idx=0):\n",
        "    reshaped = data[idx].cpu().permute(1, 2, 0)  # Adjust for RGB channels\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.title(name)\n",
        "    plt.imshow(reshaped)\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(1234)\n",
        "    train_loader, test_loader = CIFAR10_loaders()\n",
        "\n",
        "    # Update input size to [3, 32, 32] for CIFAR-10 with two hidden layers\n",
        "    net = Net([3, 32, 32]).to(device)\n",
        "    print(net)\n",
        "\n",
        "    x, y = next(iter(train_loader))\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    x_pos = overlay_y_on_x(x, y)\n",
        "    rnd = torch.randperm(x.size(0))\n",
        "    x_neg = overlay_y_on_x(x, y[rnd])\n",
        "\n",
        "    for data, name in zip([x, x_pos, x_neg], ['orig', 'pos', 'neg']):\n",
        "        visualize_sample(data, name)\n",
        "\n",
        "    net.train(x_pos, x_neg)\n",
        "\n",
        "    print('train error:', 1.0 - net.predict(x).eq(y).float().mean().item())\n",
        "\n",
        "    x_te, y_te = next(iter(test_loader))\n",
        "    x_te, y_te = x_te.to(device), y_te.to(device)\n",
        "\n",
        "    print('test error:', 1.0 - net.predict(x_te).eq(y_te).float().mean().item())"
      ],
      "metadata": {
        "id": "8sa47bhTVjs3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}