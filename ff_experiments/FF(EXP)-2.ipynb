{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDeuuzWZHon_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam, SGD\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda, RandomHorizontalFlip, RandomCrop, ColorJitter\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST, CIFAR10, FashionMNIST\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "werrzWqhH6st"
      },
      "outputs": [],
      "source": [
        "# Data Loader\n",
        "class DataLoaderManager:\n",
        "  \"\"\"\n",
        "  DataLoader manager class to handle data loading for various datasets.\n",
        "  \"\"\"\n",
        "  def __init__(self, dataset_name, train_batch_size=10000, test_batch_size=1000):\n",
        "        \"\"\"\n",
        "        Initializes the DataLoaderManager with specified dataset and batch sizes.\n",
        "\n",
        "        Args:\n",
        "        dataset_name (str): Name of the dataset ('MNIST', 'CIFAR10', 'FashionMNIST').\n",
        "        train_batch_size (int): Batch size for the training dataset.\n",
        "        test_batch_size (int): Batch size for the test dataset.\n",
        "        \"\"\"\n",
        "        self.dataset_name = dataset_name\n",
        "        self.train_batch_size = train_batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "        self.transform = self.get_transforms()\n",
        "\n",
        "  def get_transforms(self):\n",
        "        \"\"\"\n",
        "        Returns the appropriate transformation for each dataset.\n",
        "\n",
        "        Returns:\n",
        "        torchvision.transforms.Compose: Transformation pipeline.\n",
        "        \"\"\"\n",
        "        if self.dataset_name == 'CIFAR10':\n",
        "            return Compose([\n",
        "                # added image augmentation techniques in additional to normalizing\n",
        "                RandomHorizontalFlip(),\n",
        "                RandomCrop(32, padding=4),\n",
        "                ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
        "                ToTensor(),\n",
        "                Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "            ])\n",
        "        else:  # Default to MNIST and FashionMNIST normalization\n",
        "            return Compose([\n",
        "                ToTensor(),\n",
        "                Normalize((0.1307,), (0.3081,)),\n",
        "                Lambda(lambda x: torch.flatten(x))\n",
        "            ])\n",
        "\n",
        "  def load_dataset(self, train=True):\n",
        "        \"\"\"\n",
        "        Loads the specified dataset.\n",
        "\n",
        "        Args:\n",
        "        train (bool): If True, load training dataset; otherwise, load test dataset.\n",
        "\n",
        "        Returns:\n",
        "        Dataset: The requested dataset.\n",
        "        \"\"\"\n",
        "        if self.dataset_name == 'MNIST':\n",
        "            return MNIST('./data/', train=train, download=True, transform=self.transform)\n",
        "        elif self.dataset_name == 'CIFAR10':\n",
        "            return CIFAR10('./data/', train=train, download=True, transform=self.transform)\n",
        "        elif self.dataset_name == 'FashionMNIST':\n",
        "            return FashionMNIST('./data/', train=train, download=True, transform=self.transform)\n",
        "\n",
        "  def get_data_loaders(self):\n",
        "        \"\"\"\n",
        "        Creates and returns data loaders for training and testing datasets.\n",
        "\n",
        "        Returns:\n",
        "        tuple: Tuple containing the training and testing data loaders.\n",
        "        \"\"\"\n",
        "        train_loader = DataLoader(self.load_dataset(train=True),\n",
        "                                  batch_size=self.train_batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(self.load_dataset(train=False),\n",
        "                                 batch_size=self.test_batch_size, shuffle=False)\n",
        "        return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDVOZKc6HvCw"
      },
      "outputs": [],
      "source": [
        "class NegativeDataGenerator:\n",
        "    \"\"\"\n",
        "    A class to generate negative data using different methods:\n",
        "    - Long-Range vs Short-Range Correlations\n",
        "    - Hybrid Data\n",
        "    - Random Noise Corruption\n",
        "\n",
        "    This class is suitable for datasets like MNIST and CIFAR-10.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=10, noise_level=0.2):\n",
        "        \"\"\"\n",
        "        Initialize the NegativeDataGenerator class with default parameters.\n",
        "\n",
        "        Args:\n",
        "            num_classes (int): The number of classes in the dataset.\n",
        "            noise_level (float): The level of random noise to be added in random noise corruption.\n",
        "        \"\"\"\n",
        "        self.num_classes = num_classes\n",
        "        self.noise_level = noise_level\n",
        "\n",
        "    def _create_hybrid_images(self, x):\n",
        "        \"\"\"\n",
        "        Create negative examples by combining two images using a mask, preserving short-range correlations\n",
        "        but altering long-range correlations.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Batch of images (batch_size, channels, height, width).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A batch of hybrid negative images.\n",
        "        \"\"\"\n",
        "        batch_size, channels, height, width = x.shape\n",
        "        x_neg = x.clone()\n",
        "\n",
        "        # Generate a mask that has the same dimensions as each image in x\n",
        "        mask = torch.rand((1, height, width), device=x.device)  # Create a mask with only height and width dimensions\n",
        "        mask = F.avg_pool2d(mask, kernel_size=3, stride=1, padding=1)  # Use avg_pool2d for smoothing\n",
        "        mask = (mask > 0.5).float().expand(batch_size, channels, height, width)  # Expand mask across batch and channels\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            # Randomly pick another image in the batch\n",
        "            j = random.choice([idx for idx in range(batch_size) if idx != i])\n",
        "\n",
        "            # Create hybrid image by combining two images with mask and reverse of the mask\n",
        "            x_neg[i] = x[i] * mask[i] + x[j] * (1 - mask[i])\n",
        "\n",
        "        return x_neg\n",
        "\n",
        "    def _label_swapped_images(self, x, y):\n",
        "        \"\"\"\n",
        "        Generate negative examples by overlaying incorrect labels on the images.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Batch of images (batch_size, channels, height, width).\n",
        "            y (torch.Tensor): True labels corresponding to the images.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A batch of images with incorrect label overlay.\n",
        "        \"\"\"\n",
        "        y_neg = self._get_y_neg(y)  # Generate negative labels\n",
        "        x_neg = self._overlay_y_on_x(x, y_neg)  # Apply incorrect labels as overlays\n",
        "        return x_neg\n",
        "\n",
        "    def _random_noise_corruption(self, x):\n",
        "        \"\"\"\n",
        "        Generate negative examples by adding random noise to the images.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Batch of images (batch_size, channels, height, width).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A batch of corrupted images.\n",
        "        \"\"\"\n",
        "        x_neg = x.clone()\n",
        "        noise = torch.rand_like(x_neg)  # Generate random noise\n",
        "\n",
        "        # Apply noise by masking a random fraction of pixels\n",
        "        mask = (torch.rand_like(x_neg) < self.noise_level).float()  # Mask to decide where to add noise\n",
        "        x_neg = x_neg * (1 - mask) + noise * mask  # Combine original and noise based on mask\n",
        "        return x_neg\n",
        "\n",
        "    def _overlay_y_on_x(self, x, y):\n",
        "        \"\"\"\n",
        "        Overlay incorrect label information onto images.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Batch of images (batch_size, channels, height, width).\n",
        "            y (torch.Tensor): Incorrect labels to overlay on images.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Images with overlay at the position based on labels.\n",
        "        \"\"\"\n",
        "        x_ = x.clone()  # Clone the tensor to avoid modifying the original in place.\n",
        "        max_value = x.max()  # Get the maximum value from the entire tensor.\n",
        "        x_[range(x.shape[0]), :, 0, :self.num_classes] *= 0.0  # Zero out the first 'classes' pixels in the width.\n",
        "        x_[range(x.shape[0]), :, 0, y] = max_value  # Set the pixel at the label index to the maximum value.\n",
        "        return x_\n",
        "\n",
        "    def _get_y_neg(self, y):\n",
        "        \"\"\"\n",
        "        Generates negative labels by ensuring each label is different from the original.\n",
        "\n",
        "        Args:\n",
        "            y (torch.Tensor): A tensor containing the original labels.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor containing negative labels.\n",
        "        \"\"\"\n",
        "        y_neg = y.clone()\n",
        "        for idx, y_samp in enumerate(y):\n",
        "            allowed_indices = list(range(self.num_classes))\n",
        "            allowed_indices.remove(y_samp.item())\n",
        "            y_neg[idx] = torch.tensor(allowed_indices)[torch.randint(len(allowed_indices), size=(1,))].item()\n",
        "        return y_neg\n",
        "\n",
        "    def generate(self, x, y=None, method=\"long_range\"):\n",
        "        \"\"\"\n",
        "        Generate negative data based on the specified method.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Batch of images (batch_size, channels, height, width).\n",
        "            y (torch.Tensor, optional): Labels corresponding to the images (required for 'label_swap' method).\n",
        "            method (str): The method for generating negative data ('long_range', 'label_swap', 'random_noise').\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A batch of negative images.\n",
        "        \"\"\"\n",
        "        if method == \"long_range\":\n",
        "            return self._create_hybrid_images(x)\n",
        "        elif method == \"label_swap\":\n",
        "            if y is None:\n",
        "                raise ValueError(\"Labels (y) are required for the 'label_swap' method.\")\n",
        "            return self._label_swapped_images(x, y)\n",
        "        elif method == \"random_noise\":\n",
        "            return self._random_noise_corruption(x)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid method '{method}'. Choose from 'long_range', 'label_swap', or 'random_noise'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljsXPFQ3IDjz"
      },
      "outputs": [],
      "source": [
        "# Check if CUDA is available and set the default device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2vivCWfIEMe"
      },
      "outputs": [],
      "source": [
        "# Initialize data loaders\n",
        "data_manager = DataLoaderManager(\"CIFAR10\")\n",
        "train_loader, test_loader = data_manager.get_data_loaders()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hW-SQiNXIKjb"
      },
      "outputs": [],
      "source": [
        "# Fetch a single batch of data\n",
        "x, y = next(iter(train_loader))\n",
        "x, y = x.to(device), y.to(device)  # Move data to the device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxWvQxkNIM9_"
      },
      "outputs": [],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n363lGXeINiC"
      },
      "outputs": [],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRr79U0FKK1v"
      },
      "outputs": [],
      "source": [
        "def display_samples(x, x_neg, num_samples=5):\n",
        "    \"\"\"\n",
        "    Display a few samples of original and generated negative data side by side.\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor): Batch of original images.\n",
        "        x_neg (torch.Tensor): Batch of generated negative images.\n",
        "        num_samples (int): Number of samples to display.\n",
        "    \"\"\"\n",
        "    # Move data to CPU and detach from computation graph if necessary\n",
        "    x = x[:num_samples].cpu().detach()\n",
        "    x_neg = x_neg[:num_samples].cpu().detach()\n",
        "\n",
        "    fig, axes = plt.subplots(2, num_samples, figsize=(15, 4))\n",
        "    for i in range(num_samples):\n",
        "        # Display original images\n",
        "        axes[0, i].imshow(x[i].permute(1, 2, 0).squeeze(), cmap='gray')\n",
        "        axes[0, i].axis('off')\n",
        "        axes[0, i].set_title(\"Original\")\n",
        "\n",
        "        # Display negative images\n",
        "        axes[1, i].imshow(x_neg[i].permute(1, 2, 0).squeeze(), cmap='gray')\n",
        "        axes[1, i].axis('off')\n",
        "        axes[1, i].set_title(\"Negative\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1Rueq8xKPO9"
      },
      "outputs": [],
      "source": [
        "# Initialize the NegativeDataGenerator\n",
        "negative_data_generator = NegativeDataGenerator(num_classes=10, noise_level=0.2)\n",
        "\n",
        "# Generate negative data using \"long_range\" method as an example\n",
        "x_neg = negative_data_generator.generate(x, y, method=\"label_swap\")\n",
        "\n",
        "# Display original and negative samples\n",
        "display_samples(x, x_neg, num_samples=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lo1G0fQOKvy1"
      },
      "outputs": [],
      "source": [
        "# # Generate negative data using \"long_range\" method as an example\n",
        "# x_neg_1 = negative_data_generator.generate(x, y, method=\"random_noise\")\n",
        "\n",
        "# # Display original and negative samples\n",
        "# display_samples(x, x_neg_1, num_samples=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8AoOP7aU6Y9"
      },
      "outputs": [],
      "source": [
        "class PeerNormalization(nn.Module):\n",
        "    \"\"\"\n",
        "    A class for peer normalization, which normalizes the activity in a layer\n",
        "    to maintain a stable distribution of neuron activations across channels.\n",
        "\n",
        "    This normalization stabilizes neuron activity by adjusting each neuron's output\n",
        "    relative to the mean activity across all neurons in the layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PeerNormalization, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for peer normalization.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Normalized tensor.\n",
        "        \"\"\"\n",
        "        if x.dim() == 4:\n",
        "            # 4D Tensor: Normalize across channels, height, and width\n",
        "            mean_activity = x.mean(dim=(0, 2, 3), keepdim=True)\n",
        "        elif x.dim() == 2:\n",
        "            # 2D Tensor: Normalize across features\n",
        "            mean_activity = x.mean(dim=0, keepdim=True)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported tensor dimension: {x.dim()}\")\n",
        "\n",
        "        # mean_activity = x.mean(dim=(0, 2, 3), keepdim=True)  # Mean across channels, height, and width\n",
        "        x = x - mean_activity  # Subtract mean activity\n",
        "        overall_mean = mean_activity.mean()  # Calculate overall mean\n",
        "        x = x + overall_mean  # Add back overall mean to stabilize around mean activity\n",
        "        # print(f\"Shape after the Peer Normalization is {x.shape}\")\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CXV-orZMoUn"
      },
      "outputs": [],
      "source": [
        "class LocalReceptiveFieldLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A custom layer with local receptive fields (similar to a convolutional layer)\n",
        "    that uses ReLU activation and a custom training loop to distinguish between\n",
        "    positive and negative data based on a specified threshold.\n",
        "\n",
        "    Attributes:\n",
        "        conv (nn.Conv2d): Convolutional layer.\n",
        "        batch_norm (nn.BatchNorm2d): Optional batch normalization layer for stability.\n",
        "        relu (nn.ReLU): ReLU activation function.\n",
        "        opt (Adam): Optimizer for the layer parameters.\n",
        "        threshold (float): Threshold for distinguishing positive and negative samples.\n",
        "        num_epochs (int): Number of training epochs.\n",
        "        loss_values (list): List to store loss values for each training epoch.\n",
        "        g_pos_values (list): List to store goodness metric values for positive samples.\n",
        "        g_neg_values (list): List to store goodness metric values for negative samples.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1, padding='same',\n",
        "                 num_epochs=500, threshold=2.0, lr=0.03):\n",
        "        \"\"\"\n",
        "        Initializes the LocalReceptiveFieldLayer with specified hyperparameters.\n",
        "\n",
        "        Args:\n",
        "            in_channels (int): Number of input channels.\n",
        "            out_channels (int): Number of output channels.\n",
        "            kernel_size (int or tuple): Size of the convolving kernel.\n",
        "            stride (int or tuple): Stride of the convolution.\n",
        "            padding (int or tuple): Zero-padding added to both sides of the input.\n",
        "            num_epochs (int): Number of training epochs.\n",
        "            threshold (float): Threshold value to distinguish positive from negative samples.\n",
        "            lr (float): Learning rate for the optimizer.\n",
        "        \"\"\"\n",
        "        super(LocalReceptiveFieldLayer, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size\n",
        "                              ,stride=stride, padding=padding)\n",
        "        self.batch_norm = nn.BatchNorm2d(out_channels)  # Optional batch normalization\n",
        "        self.relu = nn.ReLU()\n",
        "        self.opt = Adam(self.parameters(), lr=lr)\n",
        "        self.threshold = threshold\n",
        "        self.num_epochs = num_epochs\n",
        "        self.loss_values = []\n",
        "        self.g_pos_values = []\n",
        "        self.g_neg_values = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the layer, applying batch normalization (optional)\n",
        "        and ReLU activation after the convolution.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch, channels, height, width).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after applying convolution, optional batch\n",
        "                          normalization, and ReLU activation.\n",
        "        \"\"\"\n",
        "        x = self.conv(x)\n",
        "        x = self.batch_norm(x)  # Apply batch normalization for stability\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "    def custom_train(self, x_pos, x_neg):\n",
        "        \"\"\"\n",
        "        Custom training loop to differentiate positive and negative samples based\n",
        "        on a threshold.\n",
        "\n",
        "        The loop optimizes the layer by calculating a goodness metric for both\n",
        "        positive and negative samples, with a loss function designed to push\n",
        "        the goodness above the threshold for positive samples and below for\n",
        "        negative samples.\n",
        "\n",
        "        Args:\n",
        "            x_pos (torch.Tensor): Positive samples tensor.\n",
        "            x_neg (torch.Tensor): Negative samples tensor.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Tensors containing the final forward pass outputs for positive\n",
        "                   and negative samples.\n",
        "        \"\"\"\n",
        "        for epoch in tqdm(range(self.num_epochs), desc=\"Training Progress\"):\n",
        "            # Calculate goodness metrics for positive and negative samples\n",
        "            g_pos = self.forward(x_pos).pow(2).mean(dim=[1, 2, 3])\n",
        "            g_neg = self.forward(x_neg).pow(2).mean(dim=[1, 2, 3])\n",
        "\n",
        "            # Calculate the threshold-based loss\n",
        "            loss = torch.log(1 + torch.exp(torch.cat([-g_pos + self.threshold, g_neg - self.threshold]))).mean()\n",
        "\n",
        "            # Perform backpropagation and optimization\n",
        "            self.opt.zero_grad()\n",
        "            loss.backward()\n",
        "            self.opt.step()\n",
        "\n",
        "            # Log values every 10 epochs\n",
        "            if epoch % 10 == 0:\n",
        "                self.loss_values.append(loss.item())\n",
        "                self.g_pos_values.append(g_pos.mean().item())\n",
        "                self.g_neg_values.append(g_neg.mean().item())\n",
        "\n",
        "        # Return the final forward pass outputs\n",
        "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7vGutK5U7fD"
      },
      "outputs": [],
      "source": [
        "class SimpleLayerWithPeerNormalization(nn.Module):\n",
        "    \"\"\"\n",
        "    A custom fully connected layer with peer normalization for stabilizing neuron\n",
        "    activity and a custom training loop to separate positive and negative samples\n",
        "    based on a specified threshold.\n",
        "\n",
        "    Attributes:\n",
        "        linear (nn.Linear): Fully connected layer.\n",
        "        relu (nn.ReLU): ReLU activation function.\n",
        "        peer_norm (PeerNormalization): Peer normalization layer to stabilize activations.\n",
        "        opt (Adam): Optimizer for the layer parameters.\n",
        "        threshold (float): Threshold for distinguishing positive and negative samples.\n",
        "        num_epochs (int): Number of training epochs.\n",
        "        loss_values (list): List to store loss values for each training epoch.\n",
        "        g_pos_values (list): List to store goodness metric values for positive samples.\n",
        "        g_neg_values (list): List to store goodness metric values for negative samples.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, num_epochs=500, threshold=1.5, lr=0.03):\n",
        "        \"\"\"\n",
        "        Initializes the SimpleLayerWithPeerNormalization with specified hyperparameters.\n",
        "\n",
        "        Args:\n",
        "            in_features (int): Number of input features.\n",
        "            out_features (int): Number of output features.\n",
        "            num_epochs (int): Number of training epochs.\n",
        "            threshold (float): Threshold value to separate positive and negative samples.\n",
        "            lr (float): Learning rate for the optimizer.\n",
        "        \"\"\"\n",
        "        super(SimpleLayerWithPeerNormalization, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.peer_norm = PeerNormalization()  # Apply peer normalization to stabilize activities\n",
        "        self.opt = Adam(self.parameters(), lr=lr)\n",
        "        self.threshold = threshold\n",
        "        self.num_epochs = num_epochs\n",
        "        self.loss_values = []\n",
        "        self.g_pos_values = []\n",
        "        self.g_neg_values = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the layer with peer normalization and ReLU activation.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after applying peer normalization, linear transformation,\n",
        "                          and ReLU activation.\n",
        "        \"\"\"\n",
        "        # Ensure proper flattening\n",
        "        x_flattened = x.view(x.size(0), -1)  # Flatten for batch processing\n",
        "\n",
        "        # Ensure the shape matches the expected in_features of the linear layer\n",
        "        assert x_flattened.size(1) == self.linear.in_features, (\n",
        "            f\"Flattened input size {x_flattened.size(1)} does not match expected in_features {self.linear.in_features}\"\n",
        "        )\n",
        "\n",
        "        x = self.linear(x_flattened)  # Apply linear transformation\n",
        "\n",
        "        # ReLU activation\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.peer_norm(x)  # Apply peer normalization\n",
        "\n",
        "        return x\n",
        "\n",
        "    def custom_train(self, x_pos, x_neg):\n",
        "        \"\"\"\n",
        "        Custom training loop to differentiate positive and negative samples based\n",
        "        on a threshold.\n",
        "\n",
        "        The loop optimizes the layer by calculating a goodness metric for both\n",
        "        positive and negative samples, with a loss function designed to push\n",
        "        the goodness above the threshold for positive samples and below for\n",
        "        negative samples.\n",
        "\n",
        "        Args:\n",
        "            x_pos (torch.Tensor): Positive samples tensor.\n",
        "            x_neg (torch.Tensor): Negative samples tensor.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Tensors containing the final forward pass outputs for positive\n",
        "                   and negative samples.\n",
        "        \"\"\"\n",
        "        for epoch in tqdm(range(self.num_epochs), desc=\"Training Progress\"):\n",
        "            # Calculate goodness metrics for positive and negative samples\n",
        "            g_pos = self.forward(x_pos).pow(2).mean(dim=1)\n",
        "            g_neg = self.forward(x_neg).pow(2).mean(dim=1)\n",
        "\n",
        "            # Calculate the threshold-based loss\n",
        "            loss = torch.log(1 + torch.exp(torch.cat([-g_pos + self.threshold, g_neg - self.threshold]))).mean()\n",
        "\n",
        "            # Perform backpropagation and optimization\n",
        "            self.opt.zero_grad()\n",
        "            loss.backward()\n",
        "            self.opt.step()\n",
        "\n",
        "\n",
        "            # Log values every 10 epochs\n",
        "            if epoch % 10 == 0:\n",
        "                self.loss_values.append(loss.item())\n",
        "                self.g_pos_values.append(g_pos.mean().item())\n",
        "                self.g_neg_values.append(g_neg.mean().item())\n",
        "\n",
        "        # Return the final forward pass outputs\n",
        "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFBnzoOb8v1F"
      },
      "outputs": [],
      "source": [
        "class DeepResidualNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A deep residual network with skip connections, designed for the Forward-Forward (FF) algorithm.\n",
        "    This network includes a feature extractor and classifier, with methods for forward pass,\n",
        "    prediction, and custom training.\n",
        "\n",
        "    Attributes:\n",
        "        feature_extractor (nn.ModuleList): List of layers in the feature extraction part of the network.\n",
        "        classifier (nn.ModuleList): List of layers in the classification part of the network.\n",
        "        threshold (float): Threshold for distinguishing positive and negative samples.\n",
        "        num_classes (int): Number of output classes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_channels, hidden_channels, output_features, num_blocks=6, threshold=1.5, num_classes=10):\n",
        "        \"\"\"\n",
        "        Initializes the DeepResidualNetwork with specified parameters.\n",
        "\n",
        "        Args:\n",
        "            input_channels (int): Number of input channels.\n",
        "            hidden_channels (int): Number of hidden channels in each layer.\n",
        "            output_features (int): Number of output features (classes).\n",
        "            num_blocks (int): Number of residual blocks to include.\n",
        "            threshold (float): Threshold for distinguishing positive from negative samples.\n",
        "            num_classes (int): Number of classes for classification.\n",
        "        \"\"\"\n",
        "        super(DeepResidualNetwork, self).__init__()\n",
        "        self.threshold = threshold\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Feature extractor with residual blocks\n",
        "        self.feature_extractor = nn.ModuleList()\n",
        "        self.feature_extractor.append(LocalReceptiveFieldLayer(input_channels, hidden_channels))\n",
        "        for _ in range(num_blocks):\n",
        "            self.feature_extractor.append(LocalReceptiveFieldLayer(hidden_channels, hidden_channels))\n",
        "\n",
        "        # Determine flattened feature size after feature extractor\n",
        "        dummy_input = torch.zeros(1, input_channels, 32, 32)  # CIFAR-10 size example\n",
        "        with torch.no_grad():\n",
        "            dummy_output = dummy_input\n",
        "            for layer in self.feature_extractor:\n",
        "                dummy_output = layer(dummy_output)\n",
        "            flattened_size = dummy_output.view(1, -1).size(1)\n",
        "\n",
        "        # Classifier layers\n",
        "        self.classifier = nn.ModuleList()\n",
        "        self.classifier.append(SimpleLayerWithPeerNormalization(flattened_size, hidden_channels))\n",
        "        self.classifier.append(SimpleLayerWithPeerNormalization(hidden_channels, 32))\n",
        "        self.classifier.append(nn.Linear(32, output_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network, applying each layer to the input data.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after processing through feature extractor and classifier.\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        for layer in self.feature_extractor:\n",
        "            out = layer(residual)\n",
        "            x = out + residual  # Apply residual connection\n",
        "            residual = x  # Update residual for the next layer\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten for fully connected layers\n",
        "        # print(f\"Shape after flattening: {x.shape}\")\n",
        "        for layer in self.classifier:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        Predict function to compute the goodness per label for each sample in the batch.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Predicted labels for each sample in the batch.\n",
        "        \"\"\"\n",
        "        goodness_per_label = []\n",
        "\n",
        "        for label in range(self.num_classes):  # Iterate over all possible labels\n",
        "            h = x.clone()  # Use the original data for prediction\n",
        "\n",
        "            # Pass through feature extractor\n",
        "            for layer in self.feature_extractor:\n",
        "                h = layer(h)\n",
        "\n",
        "            # Flatten before classifier\n",
        "            h = h.view(h.size(0), -1)\n",
        "\n",
        "            # Pass through classifier layers\n",
        "            for layer in self.classifier:\n",
        "                h = layer(h)\n",
        "\n",
        "            # Compute goodness for this label\n",
        "            goodness = h.pow(2).sum(dim=1)  # Goodness metric for the batch\n",
        "            goodness_per_label.append(goodness.unsqueeze(1))\n",
        "\n",
        "        # Combine goodness for all labels into one tensor\n",
        "        goodness_per_label = torch.cat(goodness_per_label, dim=1)\n",
        "\n",
        "        # Predicted labels are those with the maximum goodness\n",
        "        return goodness_per_label.argmax(dim=1)\n",
        "\n",
        "    def custom_train(self, x_pos, x_neg):\n",
        "        \"\"\"\n",
        "        Custom training method that adjusts network weights layer-by-layer based on the Forward-Forward Algorithm.\n",
        "\n",
        "        Args:\n",
        "            x_pos (torch.Tensor): Positive samples.\n",
        "            x_neg (torch.Tensor): Negative samples.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Final forward pass outputs for positive and negative samples.\n",
        "        \"\"\"\n",
        "        h_pos, h_neg = x_pos, x_neg\n",
        "        for i, layer in enumerate(list(self.feature_extractor) + list(self.classifier)):\n",
        "            if hasattr(layer, 'custom_train'):\n",
        "                print(f\"Training layer {i}: {layer.__class__.__name__}\")\n",
        "                h_pos, h_neg = layer.custom_train(h_pos, h_neg)\n",
        "            else:\n",
        "                h_pos = layer(h_pos)\n",
        "                h_neg = layer(h_neg)\n",
        "        return h_pos, h_neg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SequentialDeepNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A deep sequential network designed for the Forward-Forward (FF) algorithm.\n",
        "    This network uses a sequential stack of custom layers, including local receptive fields\n",
        "    and fully connected layers with peer normalization for feature extraction and classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_channels, hidden_channels, output_features, num_blocks=6, pool_kernel=2, num_classes=10):\n",
        "        \"\"\"\n",
        "        Initializes the SequentialDeepNetwork with specified parameters.\n",
        "\n",
        "        Args:\n",
        "            input_channels (int): Number of input channels.\n",
        "            hidden_channels (int): Number of hidden channels in each layer.\n",
        "            output_features (int): Number of output features (classes).\n",
        "            num_blocks (int): Number of blocks in the feature extraction layers.\n",
        "            pool_kernel (int): Size of the max-pooling kernel.\n",
        "            num_classes (int): Number of classes for classification.\n",
        "        \"\"\"\n",
        "        super(SequentialDeepNetwork, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Feature extraction layers\n",
        "        layers = []\n",
        "        layers.append(LocalReceptiveFieldLayer(input_channels, hidden_channels))\n",
        "        layers.append(nn.AdaptiveAvgPool2d((16,16)))  # Max-pooling layer\n",
        "        for _ in range(num_blocks - 1):\n",
        "            layers.append(LocalReceptiveFieldLayer(hidden_channels, hidden_channels))\n",
        "            layers.append(nn.AdaptiveAvgPool2d((8,8)))  # Max-pooling layer\n",
        "\n",
        "        layers.append(LocalReceptiveFieldLayer(hidden_channels, hidden_channels))\n",
        "        layers.append(nn.AdaptiveAvgPool2d((4, 4)))\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(*layers)\n",
        "\n",
        "        # Determine the flattened feature size dynamically\n",
        "        # dummy_input = torch.zeros(1, input_channels, 32, 32)  # Example input size for CIFAR-10\n",
        "        # with torch.no_grad():\n",
        "        #     dummy_output = self.feature_extractor(dummy_input)\n",
        "        #     flattened_size = dummy_output.view(1, -1).size(1)\n",
        "\n",
        "        # Classification layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            SimpleLayerWithPeerNormalization(hidden_channels*4*4, hidden_channels),\n",
        "            nn.Dropout(0.2),\n",
        "            SimpleLayerWithPeerNormalization(hidden_channels, 32),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(32, output_features)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor after processing through feature extractor and classifier.\n",
        "        \"\"\"\n",
        "        x = self.feature_extractor(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten for fully connected layers\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        Predict function to compute the goodness per label for each sample in the batch.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Predicted labels for each sample in the batch.\n",
        "        \"\"\"\n",
        "        goodness_per_label = []\n",
        "\n",
        "        for label in range(self.num_classes):  # Iterate over all possible labels\n",
        "            # Overlay label information on the input\n",
        "            data_generator = NegativeDataGenerator(num_classes=self.num_classes)\n",
        "            y = torch.full((x.size(0),), label, dtype=torch.long, device=x.device)\n",
        "            h = data_generator._overlay_y_on_x(x, y)\n",
        "\n",
        "            # Pass through feature extractor\n",
        "            h = self.feature_extractor(h)\n",
        "\n",
        "            # Flatten before classifier\n",
        "            h = h.view(h.size(0), -1)\n",
        "\n",
        "            # Pass through classifier layers\n",
        "            h = self.classifier(h)\n",
        "\n",
        "            # Compute goodness for this label\n",
        "            goodness = h.pow(2).sum(dim=1)  # Goodness metric for the batch\n",
        "            goodness_per_label.append(goodness.unsqueeze(1))\n",
        "\n",
        "        # Combine goodness for all labels into one tensor\n",
        "        goodness_per_label = torch.cat(goodness_per_label, dim=1)\n",
        "\n",
        "        # Predicted labels are those with the maximum goodness\n",
        "        return goodness_per_label.argmax(dim=1)\n",
        "\n",
        "    def custom_train(self, x_pos, x_neg):\n",
        "        \"\"\"\n",
        "        Custom training method that adjusts network weights layer-by-layer based on the Forward-Forward Algorithm.\n",
        "\n",
        "        Args:\n",
        "            x_pos (torch.Tensor): Positive samples.\n",
        "            x_neg (torch.Tensor): Negative samples.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Final forward pass outputs for positive and negative samples.\n",
        "        \"\"\"\n",
        "        h_pos, h_neg = x_pos, x_neg\n",
        "        for i, layer in enumerate(list(self.feature_extractor) + list(self.classifier)):\n",
        "            if hasattr(layer, 'custom_train'):\n",
        "                print(f\"Training layer {i}: {layer.__class__.__name__}\")\n",
        "                h_pos, h_neg = layer.custom_train(h_pos, h_neg)\n",
        "            else:\n",
        "                h_pos = layer(h_pos)\n",
        "                h_neg = layer(h_neg)\n",
        "        return h_pos, h_neg\n"
      ],
      "metadata": {
        "id": "qNTUkk2BLkkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFh9B1VT80LK"
      },
      "outputs": [],
      "source": [
        "input_channels = 3  # CIFAR-10-like data\n",
        "hidden_channels = 64\n",
        "output_features = 10\n",
        "num_blocks = 8\n",
        "\n",
        "Net = SequentialDeepNetwork(input_channels, hidden_channels, output_features, num_blocks=num_blocks)\n",
        "Net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(Net, input_size=(32, 3, 32, 32))"
      ],
      "metadata": {
        "id": "_Buh3Xev0MWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reducing the pixel information for better visibility of patterns to model\n",
        "x, x_neg = x / 255.0, x_neg / 255.0"
      ],
      "metadata": {
        "id": "F6JGtah4p9VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hESgclVv-K3l"
      },
      "outputs": [],
      "source": [
        "Net.custom_train(x, x_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E72fEQH2FQHn"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "torch.save(Net.state_dict(), \"ff_sequential_model.pth\")\n",
        "print(\"Model saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved state_dict\n",
        "Net.load_state_dict(torch.load(\"ff_sequential_model.pth\",weights_only=True))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "Net.eval()"
      ],
      "metadata": {
        "id": "KNbrUQ0tmYtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iuxIgxj6mm_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvdxkvhBnACu"
      },
      "outputs": [],
      "source": [
        "def calculate_training_error(Net, train_loader, device):\n",
        "    \"\"\"\n",
        "    Calculate the training error for the given network using the predict method.\n",
        "\n",
        "    Args:\n",
        "        Net (nn.Module): The trained network.\n",
        "        train_loader (DataLoader): DataLoader for the training dataset.\n",
        "        device (torch.device): Device to perform computations on (CPU or GPU).\n",
        "\n",
        "    Returns:\n",
        "        float: The training error as a percentage.\n",
        "    \"\"\"\n",
        "    Net.eval()  # Set the network to evaluation mode\n",
        "    total_samples = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():  # No gradient computation during evaluation\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)  # Move data to device\n",
        "            predictions = Net.predict(x_batch)  # Use the predict method\n",
        "            correct_predictions += (predictions == y_batch).sum().item()\n",
        "            total_samples += y_batch.size(0)\n",
        "\n",
        "    training_error = 1 - (correct_predictions / total_samples)\n",
        "    return training_error * 100  # Return as a percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4aRE5U8otj1"
      },
      "outputs": [],
      "source": [
        "1.0 - Net.predict(x[0:6]).eq(y[0:6]).float().mean().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXPFaPA8nBjI"
      },
      "outputs": [],
      "source": [
        "training_error = calculate_training_error(Net, train_loader, device)\n",
        "print(f\"Training Error: {training_error:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEIQq6YmoNMS"
      },
      "outputs": [],
      "source": [
        "# Class names for CIFAR-10\n",
        "class_names = [\n",
        "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
        "]\n",
        "\n",
        "# Store correct and incorrect predictions\n",
        "correct_preds = []\n",
        "incorrect_preds = []\n",
        "\n",
        "# Run inference and collect predictions\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        predictions = Net.predict(images)\n",
        "\n",
        "        # Compare predictions with true labels\n",
        "        for i in range(len(labels)):\n",
        "            if predictions[i] == labels[i]:\n",
        "                correct_preds.append((images[i].cpu(), labels[i].cpu()))\n",
        "            else:\n",
        "                incorrect_preds.append((images[i].cpu(), predictions[i].cpu(), labels[i].cpu()))\n",
        "\n",
        "# Visualize Correct Predictions\n",
        "def visualize_correct_predictions(correct_preds, num_samples=5):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for i in range(num_samples):\n",
        "        image, label = correct_preds[i]\n",
        "        plt.subplot(1, num_samples, i + 1)\n",
        "        plt.imshow(image.permute(1, 2, 0))  # Convert to HWC format\n",
        "        plt.title(f\"Correct: {class_names[label]}\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# Visualize Incorrect Predictions\n",
        "def visualize_incorrect_predictions(incorrect_preds, num_samples=5):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for i in range(num_samples):\n",
        "        image, predicted, true = incorrect_preds[i]\n",
        "        plt.subplot(1, num_samples, i + 1)\n",
        "        plt.imshow(image.permute(1, 2, 0))  # Convert to HWC format\n",
        "        plt.title(f\"Pred: {class_names[predicted]}\\nTrue: {class_names[true]}\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# Display results\n",
        "print(\"Correct Predictions:\")\n",
        "visualize_correct_predictions(correct_preds, num_samples=7)\n",
        "\n",
        "print(\"Incorrect Predictions:\")\n",
        "visualize_incorrect_predictions(incorrect_preds, num_samples=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip_dZ_QlruEG"
      },
      "source": [
        "# Some Manual Debugging of Network Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxpWSW-q-gpz"
      },
      "outputs": [],
      "source": [
        "# Function to visualize and trace tensor shapes\n",
        "def test_feature_extractor_with_real_data(Net, sample_image):\n",
        "    \"\"\"\n",
        "    Test the feature extractor using a real sample image and trace the tensor shapes.\n",
        "\n",
        "    Args:\n",
        "        Net (nn.Module): The network object containing the feature extractor and classifier.\n",
        "        sample_image (torch.Tensor): A single sample image from the dataset.\n",
        "    \"\"\"\n",
        "    print(\"Input image shape:\", sample_image.shape)\n",
        "\n",
        "    # Visualize the input image\n",
        "    plt.imshow(sample_image.permute(1, 2, 0).cpu().numpy())  # Adjust permutation for visualization\n",
        "    plt.title(\"Input Image\")\n",
        "    plt.show()\n",
        "\n",
        "    # Send the image through the feature extractor\n",
        "    x = sample_image.unsqueeze(0)  # Add batch dimension\n",
        "    for i, layer in enumerate(Net.feature_extractor):\n",
        "        x = layer(x)\n",
        "        print(f\"Shape after feature extractor layer {i}: {x.shape}\")\n",
        "\n",
        "    # Flatten the output and check shape\n",
        "    x_flattened = x.view(x.size(0), -1)\n",
        "    print(\"Shape after flattening:\", x_flattened.shape)\n",
        "\n",
        "    # Pass through the classifier\n",
        "    for i, layer in enumerate(Net.classifier):\n",
        "        x_flattened = layer(x_flattened)\n",
        "        print(f\"Shape after classifier layer {i}: {x_flattened.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3MfYaFnr5el"
      },
      "outputs": [],
      "source": [
        "# Select a sample image\n",
        "sample_image, label = x[0], y[0]  # Select the first image in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQq6NZbCr8Wi"
      },
      "outputs": [],
      "source": [
        "sample_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9b3PLDusMXx"
      },
      "outputs": [],
      "source": [
        "label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjScGzMisOsC"
      },
      "outputs": [],
      "source": [
        "# Test the feature extractor with a real image\n",
        "test_feature_extractor_with_real_data(Net, sample_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEnF45jSsS3R"
      },
      "outputs": [],
      "source": [
        "lab = labels.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter(lab)"
      ],
      "metadata": {
        "id": "a5nLJbeew7UQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EfvJhy4Ox0ZK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}